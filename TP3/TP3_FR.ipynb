{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF 8215 - Intelligence artif.: méthodes et algorithmes \n",
    "## Automne 2018 - TP3 - Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date de rendu: 12 Décembre**\n",
    "\n",
    "**Fichiers à rendre:**\n",
    "    * TP3_FR.ipynb complété\n",
    "    * SoftmaxClassifier.py complété\n",
    "    * test_prediction.csv le fichier de résultat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Le but de ce TP est de vous donner un aperçu du déroulement général d'un projet de machine learning tout en vous familiarisant avec des librairies python adaptées.\n",
    "\n",
    "\n",
    "Dans la première partie, vous implémenterez un algorithme de classification multiclasse appelé **softmax regression** à l'aide uniquement de la bibliothèque **numpy** et l'intégrerez à la bibliothèque **scikit-learn**.\n",
    "\n",
    "Dans la deuxième partie, vous prendrez connaissance du **dataset** utilisé pour ce projet. Et vous serez amenés à effectuer le **preprocessing** de ces données pour qu'elles soient utilisables dans les algorithmes de machine learning classiques. Vous utiliserez les bibliothèques **pandas** et **scikit-learn**.\n",
    "\n",
    "Enfin, dans la troisième partie, vous comparerez l'efficacité du modèle que vous avez implémenté avec d'autres modèles déjà implémentés dans **sklearn**. Puis vous tenterez d'améliorer les performances de l'algorithme sélectionné.\n",
    "\n",
    "Pour enfin soumettre vos résultats sur la plateforme **kaggle**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Pour installer **pandas** et **scikit-learn** le plus simple est de télécharger et d'installer **Anaconda** qui regroupe les packages les plus utilisés pour le calcul scientifique et la science des données.\n",
    "\n",
    "Vous trouverez la distribution ici : https://www.anaconda.com/download/#linux .\n",
    "\n",
    "Assurez-vous d'avoir la version **20.0** de **scikit-learn**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 1: Compétition (2 points)\n",
    "\n",
    "Quand vous aurez terminé le TP, vous pourrez soumettre vos prédictions sur **kaggle**, vous obtiendrez votre performance en terme de **log loss**.\n",
    "Vous pouvez ensuite me communiquer ce résultat par mail (laurent.boucaud@polymtl.ca) et me joindre votre fichier de prédiction sur l'ensemble de test(pour vérification).\n",
    "\n",
    "Une conversation dans le forum sera créée pour tenir à jour le meilleur score obtenu par une des équipes du cours.\n",
    "\n",
    "Tant qu'aucun forum n'est créé, **ne m'envoyez pas vos performances si elles sont supérieures à 0.8 de log loss**.\n",
    "\n",
    "Une fois le premier meilleur score affiché dans le forum, **ne me communiquez vos résultats que si votre log loss est inférieure au précédent meilleur score**.\n",
    "\n",
    "Le nombre de points obtenus sera proportionnel au classement des équipes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Softmax Regression (10 points)\n",
    "\n",
    "Dans cette partie vous implémenterez **softmax regression** la variante de **logistic regression** qui permet d'effectuer de la classification pour un nombre de classe supérieur à 2.\n",
    "\n",
    "Le code à compléter se trouve dans le fichier **SoftmaxClassifier.py**. \n",
    "\n",
    "**Pour cet exercice, la contrainte est d'utiliser uniquement la bibliothèque numpy**\n",
    "\n",
    "## Encapsulation avec sklearn\n",
    "\n",
    "La classe **SoftmaxClassifier** hérite des classes **BaseEstimator** et **ClassifierMixin** de **scikit-learn** ce qui nous permettra d'utiliser facilement avec notre classifier les outils fournis par scikit-learn dans la suite du TP.\n",
    "\n",
    "Pour la compatibilité, le classifier implémente obligatoirement les méthodes:\n",
    "\n",
    "* **fit**: responsable de l'entraînement du modèle\n",
    "* **predict_proba**: permet de prédire la probabilité de chaque classe pour chaque exemple du dataset fourni.\n",
    "* **predict**: permet de prédire la classe pour chaque exemple du dataset fourni.\n",
    "* **score**: permet de quantifier l'écart entre les classes prédites et les classes réelles pour le dataset fourni\n",
    "\n",
    "\n",
    "## Train/Test set:\n",
    "\n",
    "Quand on veut tester les performances de l'apprentissage d'un algorithme de machine learning, on **ne le teste pas sur les données utilisées pour l'apprentissage**.\n",
    "\n",
    "En effet, ce qui nous intéresse c'est que notre algorithme soit **capable de généraliser** ses prédictions à des données qu'il n'a **jamais vu**.\n",
    "\n",
    "Pour illustrer, si on teste un algorithme sur les données d'entrainement, on teste sa capacité à **apprendre par coeur** le dataset et non à **généraliser**.\n",
    "\n",
    "Par conséquent, quand on reçoit un nouveau dataset, la première chose à faire et de le **diviser en deux parties**: un ensemble d'**entraînement** (**70-80%** du dataset) et un ensemble de **test**(**20-30%** du dataset).\n",
    "\n",
    "Tous les algorithmes de **traitement des données** et d'apprentissage devront être appris uniquement sur l'ensemble d'entraînement et appliqués ensuite sur l'ensemble de test.\n",
    "\n",
    "Cela garantit l'absence de connaissances préalables de l'ensemble de test lors de l'entrainement.\n",
    "\n",
    "## Gradient descent\n",
    "\n",
    "La descente de gradient est un algorithme qui permet trouver la solution optimale d'un certains nombre de problèmes. Le principe est le suivant: on définit une **fonction de coût J**  qui caractérise le problème.\n",
    "Cette fonction dépend d'un ensemble de **paramètres $\\theta$ **. La descente de gradient cherche à **minimiser** la fonction de coût en **modifiant itérativement** les paramètres.\n",
    "\n",
    "### Gradient\n",
    "\n",
    "Le gradient de la fonction de coûts pour un $\\theta$ donné, correspond à la direction dans laquelle il faut modifier $\\theta$ pour réduire la valeur de la fonction de coût. \n",
    "\n",
    "La fonction de coût est minimale quand le gradient est nul.\n",
    "\n",
    "Concrètement, on initialize $\\theta$ aléatoirement, et on effectue à chaque itération un pas pour réduire la fonction de coût jusqu'à convergence de l'algorithme à un minimum.\n",
    "\n",
    "### Learning rate\n",
    "\n",
    "Le taux d'apprentissage correspond à la taille du pas que l'on va effectuer dans la direction du gradient.\n",
    "Plus il est grand, plus la convergence est rapide mais il y a un risque que l'algorithme diverge.\n",
    "\n",
    "Plus il est petit, plus la convergence est lente.\n",
    "\n",
    "### Batch gradient descent\n",
    "\n",
    "Il existe plusieurs algorithmes de descente de gradient. Nous utiliserons Batch gradient descent.\n",
    "\n",
    "Dans cet algorithme, avant de mettre à jour $\\theta$, on calcule les gradients sur l'ensemble des exemples d'entraînement.\n",
    "\n",
    "### Epoch\n",
    "\n",
    "Il s'agit d'un pas de la descente de gradient, soit une unique mise à jour de gradient.\n",
    "\n",
    "### Bias/Variance tradeoff\n",
    "\n",
    "Lorsqu'on entraine un algorithme de machine learning on cherche un équilibre entre **biais** et **variance**.\n",
    "\n",
    "Un modèle avec un **biais fort**, est un modèle qui est **trop simple** pour la structure donnée considérée (modèle linéaire pour données quadratiques), cela limite la capacité du modèle à généraliser. On appelle aussi le biais **underfitting**.\n",
    "\n",
    "Un modèle avec une **variance élevée** signifie qu'il est sensible aux petites variations dans les données d'entrainement, cela correspond à l'**overfitting**, c'est-à-dire que le modèle est trop proche de la structure de l'ensemble d'entrainement ce qui **limite sa capacité à généraliser**.\n",
    "\n",
    "Un modèle avec un **biais important** aura une **mauvaise performance** sur l'ensemble d'**entraînement**.\n",
    "Un modèle avec une **variance importante** aura une performance bien **moins bonne** sur l'ensemble de **test** que sur l'ensemble d'**entrainement**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot-encoding\n",
    "\n",
    "En machine learning pour représenter un vecteur de données catégoriques, on utilise le one-hot encoding.\n",
    "\n",
    "Pour un vecteur comportant 5 exemples et 3 catégories différentes, on le représente sous forme d'une matrice de taille 5 par 3. Cette matrice est entièrement remplie de 0 sauf à l'indice correspondant au numéro de la classe pour chaque exemple.\n",
    "\n",
    "\n",
    "Par exemple\n",
    "$ y = \\left(\\begin{array}{cc} \n",
    "1 \\\\\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "2 \\\\\n",
    "\\end{array}\\right) $\n",
    "\n",
    "devient:\n",
    "\n",
    "$ yohe =  \\left(\\begin{array}{cc} \n",
    "1. & 0. & 0.\\\\\n",
    "1. & 0. & 0.\\\\\n",
    "0. & 1. & 0.\\\\\n",
    "0. & 0. & 1.\\\\\n",
    "0. & 1. & 0.\\\\\n",
    "\\end{array}\\right) $\n",
    "\n",
    "\n",
    "#### Question 1 (1 point)\n",
    "Implémentez  la fonction  **_one_hot**  dans SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from SoftmaxClassifier import SoftmaxClassifier\n",
    "import numpy as np\n",
    "\n",
    "softmax = SoftmaxClassifier(use_zero_indexed_classes = False)\n",
    "softmax.nb_classes = 3 # On ne fait pas appel à fit, donc on doit spécifier le nombre de classes\n",
    "softmax._one_hot(np.array([1,1,2,3,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice de poids\n",
    "\n",
    "Soit $ X_{m * n} $ la matrice d'exemple et $ \\Theta _{n*K} $ la matrice de poids avec:\n",
    "\n",
    "* **m** le nombre d'exemples\n",
    "* **n** le nombre de features\n",
    "* **k** le nombre de classes\n",
    "\n",
    "Il est d'usage d'ajouter une colonne supplémentaire à X, cette colonne est remplie de 1. Pour prendre en compte ce changement, il faut rajouter une ligne à la matrice $\\Theta$.\n",
    "\n",
    "On obtient X_bias$_{m*(n+1)}$ et $ \\Theta _{(n+1)*K} $\n",
    "\n",
    "\n",
    "Intuitivement, à chaque classe K est associée une colonne de $\\theta$.\n",
    "\n",
    "On note $\\theta_k$ le vecteur de dimension n+1 la colonne de poids associée à la prédiction de la classe k.\n",
    "\n",
    "$\\Theta$ = [$\\theta_0$,$\\theta_1$... $\\theta_k$ ... $\\theta_n$ ]\n",
    "\n",
    "Ainsi $ z = x * \\Theta $ donne un vecteur de dimension K qui correspond aux **logits** associés à x pour chacune des classes.\n",
    "\n",
    "#### Question 2 (1 point)\n",
    "Dans la fonction  **fit**  dans SoftmaxClassifier.py instanciez X_bias et initialisez $\\Theta$ aléatoirement. (ligne 74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoftmaxClassifier(alpha=100, early_stopping=True, eps=1e-05, lr=0.1,\n",
       "         n_epochs=1000, regularization=True, threshold=1e-10,\n",
       "         use_zero_indexed_classes=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = SoftmaxClassifier(use_zero_indexed_classes = True)\n",
    "softmax.fit(np.zeros((5, 5)), y=np.array([0, 1, 2, 3, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "On veut convertir le vecteur de logits **z** obtenu dans la partie précédente, en un **vecteur de probabilité**.\n",
    "\n",
    "Pour cela on définit la **fonction softmax**:\n",
    "\n",
    "$$ \\hat{p_x}^k = softmax(z)_k = \\frac{exp(z_k)}{\\sum_{\\substack{1<j<K}} exp(z_j)} $$\n",
    "\n",
    "Intuitivement, pour un logit de z, $z_k$, on prend l'exponentielle de cette valeur et on la divise par la somme des exponentielles de chaque logit du vecteur **z**. On obtient  $\\hat{p_x}^k$ la probabilité que l'exemple **x** appartienne à la classe **k**.\n",
    "\n",
    "On réitère l'opération pour chaque logit du vecteur **z**. \n",
    "\n",
    "On obtient ainsi un vecteur de probabilités $\\hat{p_x}$ pour un exemple **x**. \n",
    "\n",
    "La division permet de rendre la somme des termes du vecteur $\\hat{p_x}$ égale à 1 ce qui est indispensable dans le cadre des probabilités.\n",
    "\n",
    "#### Question 3 (1 point)\n",
    "Implémentez  la fonction  **_softmax**  dans SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20070649, 0.24514346, 0.27092543, 0.14868703, 0.13453759])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax._softmax(np.array([0.5, 0.7, 0.8, 0.2, 0.1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 (1 point)\n",
    "En utilisant la fonction **_softmax** de la question 3, implémentez  les fonctions  **predict_proba** et **predict**  dans SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.rand(5,5)\n",
    "y = np.array([0, 1, 2, 3, 4])\n",
    "predict_proba = softmax.predict_proba(x)\n",
    "softmax.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction de coût Log loss\n",
    "\n",
    "Soit la fonction de coût log loss (ou cross entropy):\n",
    "\n",
    "$$ J( \\Theta) = \\frac{-1}{m}\\sum_{\\substack{1<i<m}} \\sum_{\\substack{1<k<K}} y_k^i log( \\hat{p_k}^i ) $$\n",
    "\n",
    "avec:\n",
    "* **K** le nombre de classes\n",
    "* **m** le nombre d'exemples dans les données\n",
    "* $ \\hat{p_k}^i  $  la probabilité que l'exemple i soit de la classe k\n",
    "* $y_k^i$ vaut 1 si la classe cible de l'exemple i est k, 0 sinon\n",
    "\n",
    "**Détail d'implémentation:** La fonction n'est pas définie pour des valeurs de probabilité de 0. ou 1., il faut donc s'assurer que étant donné $\\epsilon$, les probabilités sont comprises dans [$\\epsilon$, 1. - $\\epsilon$].\n",
    "#### Question 5 (1 point)\n",
    "Implémentez  la fonction  **_cost_function**  dans SoftmaxClassifier.py en prenant en compte le **détail d'implémentation** (variable self.eps) et utilisez-la pour calculer la variable **loss** dans la fonction **fit** (ligne 84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.24482835212462"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax._cost_function(predict_proba, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient de la fonction de coût\n",
    "\n",
    "Le **gradient de J** par rapport à la classe k (par rapport à $\\theta_k$) est :\n",
    "\n",
    "\n",
    "$$ \\Delta_{\\theta_k}J( \\Theta) = \\frac{1}{m} \\sum_{\\substack{1<i<m}}( \\hat{p_k}^i - y_k^i)x^i  $$\n",
    "\n",
    "avec:\n",
    "* **K** le nombre de classes\n",
    "* **m** le nombre d'exemples dans les données\n",
    "* $ \\hat{p_k}^i  $  la probabilité que l'exemple i soit de la classe k\n",
    "* $y_k^i$ vaut 1 si la classe cible de l'exemple i est k, 0 sinon\n",
    "\n",
    "Sous **forme matricielle**, on peut écrire le **gradient de J par rapport à $\\Theta$**:\n",
    "$$ \\Delta_J( \\Theta) = \\frac{1}{m} X_{bias}^T *( \\hat{p} - y_{ohe}) $$\n",
    "\n",
    "avec:\n",
    "* $\\hat{p}$ la matrice de probabilité prédite pour chaque example et pour chaque classe\n",
    "* $y_{ohe}$ la version one-hot de y\n",
    "* $X_{bias}^T$  la matrice transposée de $X_{bias}$\n",
    "* **\\*** le produit matriciel\n",
    "\n",
    "#### Question 6 (1 point)\n",
    "Implémentez  la fonction  **_get_gradient**  dans SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.74174007e-02, -1.07504660e-01,  1.20846716e-01,\n",
       "         1.04843175e-02, -5.12437743e-02],\n",
       "       [ 5.09271206e+00, -3.33067722e+00,  1.08180157e+01,\n",
       "         4.76524150e+00, -2.08718187e+00],\n",
       "       [ 1.38363227e+00, -2.56167618e+00,  5.36735566e+00,\n",
       "        -3.93120321e+00, -5.75125987e+00],\n",
       "       [ 1.17153579e+01,  4.96572494e+00,  3.17703305e+00,\n",
       "         6.18494133e-01, -1.97766116e+00],\n",
       "       [ 1.25548412e+01, -1.11648189e+01, -8.82944016e-01,\n",
       "        -1.30641684e+01, -1.02864609e+01],\n",
       "       [ 9.04613982e-01,  4.63774026e+00,  3.57120546e+00,\n",
       "         3.51928530e+00,  3.88608978e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bias = np.concatenate((x, np.ones((x.shape[0], 1))), axis=1)\n",
    "softmax._get_gradient(X_bias, y, predict_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mise à jour des poids\n",
    "\n",
    "Quand le gradient a été calculé, il faut mettre à jour les poids avec ces gradients.\n",
    "\n",
    "$$ \\Theta  = \\Theta - \\gamma \\Delta J( \\Theta) $$\n",
    "\n",
    "\n",
    "avec:\n",
    "* $\\Theta$ la matrice de poids\n",
    "* $\\gamma$  le taux d'apprentissage\n",
    "* $\\Delta J( \\Theta)$ le gradient de $J( \\Theta)$ selon $\\Theta$\n",
    "\n",
    "#### Question 7 (1 point)\n",
    "Mettez à jour la variable **self.theta_** dans la fonction **fit**  dans SoftmaxClassifier.py (ligne 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.117526999148857,\n",
       " 1.1119668344723974,\n",
       " 1.106693026120432,\n",
       " 1.1016848901779177,\n",
       " 1.0969233701100638,\n",
       " 1.0923909038491566,\n",
       " 1.0880713015545114,\n",
       " 1.0839496333921936,\n",
       " 1.080012126651488,\n",
       " 1.0762460715104978,\n",
       " 1.0726397347765435,\n",
       " 1.0691822809526694,\n",
       " 1.0658637000151856,\n",
       " 1.0626747413255664,\n",
       " 1.0596068531407128,\n",
       " 1.05665212722684,\n",
       " 1.05380324812279,\n",
       " 1.0510534466375694,\n",
       " 1.0483964572037987,\n",
       " 1.0458264787432425,\n",
       " 1.0433381387324823,\n",
       " 1.0409264601860997,\n",
       " 1.038586831301476,\n",
       " 1.0363149775336125,\n",
       " 1.034106935890357,\n",
       " 1.0319590312582934,\n",
       " 1.029867854587404,\n",
       " 1.027830242778717,\n",
       " 1.02584326013358,\n",
       " 1.0239041812362082,\n",
       " 1.0220104751527785,\n",
       " 1.0201597908408506,\n",
       " 1.0183499436722854,\n",
       " 1.0165789029813195,\n",
       " 1.014844780557084,\n",
       " 1.0131458200067347,\n",
       " 1.0114803869215905,\n",
       " 1.0098469597842867,\n",
       " 1.0082441215600568,\n",
       " 1.0066705519198744,\n",
       " 1.00512502004739,\n",
       " 1.003606377985422,\n",
       " 1.0021135544812563,\n",
       " 1.0006455492931787,\n",
       " 0.9992014279235975,\n",
       " 0.9977803167467649,\n",
       " 0.9963813985015598,\n",
       " 0.9950039081220418,\n",
       " 0.9936471288805478,\n",
       " 0.992310388820005,\n",
       " 0.9909930574538854,\n",
       " 0.9896945427138341,\n",
       " 0.9884142881265061,\n",
       " 0.9871517702025061,\n",
       " 0.985906496021604,\n",
       " 0.9846780009995755,\n",
       " 0.9834658468230948,\n",
       " 0.9822696195401166,\n",
       " 0.9810889277941197,\n",
       " 0.9799234011914281,\n",
       " 0.9787726887916488,\n",
       " 0.9776364577119774,\n",
       " 0.9765143918368275,\n",
       " 0.9754061906248634,\n",
       " 0.9743115680061032,\n",
       " 0.9732302513623114,\n",
       " 0.9721619805843942,\n",
       " 0.9711065072009863,\n",
       " 0.9700635935728484,\n",
       " 0.9690330121480942,\n",
       " 0.9680145447736395,\n",
       " 0.9670079820586133,\n",
       " 0.9660131227857838,\n",
       " 0.9650297733673543,\n",
       " 0.9640577473417503,\n",
       " 0.963096864908286,\n",
       " 0.9621469524968149,\n",
       " 0.9612078423697055,\n",
       " 0.9602793722536697,\n",
       " 0.9593613849991678,\n",
       " 0.9584537282652806,\n",
       " 0.9575562542281034,\n",
       " 0.9566688193108589,\n",
       " 0.9557912839340698,\n",
       " 0.9549235122842519,\n",
       " 0.9540653720997094,\n",
       " 0.9532167344721223,\n",
       " 0.9523774736627109,\n",
       " 0.9515474669318646,\n",
       " 0.9507265943811984,\n",
       " 0.9499147388070823,\n",
       " 0.9491117855647665,\n",
       " 0.9483176224422878,\n",
       " 0.9475321395434022,\n",
       " 0.9467552291788617,\n",
       " 0.9459867857653803,\n",
       " 0.9452267057317141,\n",
       " 0.9444748874312985,\n",
       " 0.9437312310609485,\n",
       " 0.9429956385851513,\n",
       " 0.9422680136655265,\n",
       " 0.9415482615950609,\n",
       " 0.9408362892367453,\n",
       " 0.9401320049662873,\n",
       " 0.9394353186185811,\n",
       " 0.9387461414376523,\n",
       " 0.9380643860298142,\n",
       " 0.9373899663197888,\n",
       " 0.9367227975095695,\n",
       " 0.9360627960398192,\n",
       " 0.9354098795536101,\n",
       " 0.9347639668623315,\n",
       " 0.9341249779136007,\n",
       " 0.9334928337610272,\n",
       " 0.9328674565356949,\n",
       " 0.9322487694192284,\n",
       " 0.931636696618333,\n",
       " 0.9310311633406945,\n",
       " 0.9304320957721388,\n",
       " 0.9298394210549642,\n",
       " 0.9292530672673546,\n",
       " 0.9286729634037987,\n",
       " 0.9280990393564406,\n",
       " 0.9275312258972999,\n",
       " 0.9269694546612914,\n",
       " 0.9264136581299947,\n",
       " 0.9258637696161177,\n",
       " 0.9253197232486073,\n",
       " 0.9247814539583601,\n",
       " 0.9242488974644945,\n",
       " 0.923721990261146,\n",
       " 0.9232006696047477,\n",
       " 0.9226848735017669,\n",
       " 0.9221745406968669,\n",
       " 0.9216696106614657,\n",
       " 0.9211700235826661,\n",
       " 0.9206757203525343,\n",
       " 0.9201866425577061,\n",
       " 0.9197027324692978,\n",
       " 0.9192239330331067,\n",
       " 0.9187501878600806,\n",
       " 0.9182814412170439,\n",
       " 0.9178176380176635,\n",
       " 0.91735872381364,\n",
       " 0.9169046447861163,\n",
       " 0.9164553477372841,\n",
       " 0.9160107800821878,\n",
       " 0.9155708898407051,\n",
       " 0.9151356256297044,\n",
       " 0.9147049366553636,\n",
       " 0.9142787727056475,\n",
       " 0.913857084142932,\n",
       " 0.9134398218967719,\n",
       " 0.9130269374568047,\n",
       " 0.9126183828657832,\n",
       " 0.9122141107127335,\n",
       " 0.9118140741262335,\n",
       " 0.9114182267678027,\n",
       " 0.9110265228254075,\n",
       " 0.9106389170070679,\n",
       " 0.9102553645345705,\n",
       " 0.9098758211372773,\n",
       " 0.9095002430460333,\n",
       " 0.9091285869871637,\n",
       " 0.9087608101765616,\n",
       " 0.9083968703138652,\n",
       " 0.9080367255767136,\n",
       " 0.9076803346150913,\n",
       " 0.9073276565457473,\n",
       " 0.9069786509466935,\n",
       " 0.9066332778517792,\n",
       " 0.9062914977453385,\n",
       " 0.9059532715569102,\n",
       " 0.9056185606560272,\n",
       " 0.9052873268470752,\n",
       " 0.9049595323642188,\n",
       " 0.9046351398663913,\n",
       " 0.9043141124323523,\n",
       " 0.9039964135558054,\n",
       " 0.9036820071405789,\n",
       " 0.9033708574958674,\n",
       " 0.9030629293315321,\n",
       " 0.902758187753461,\n",
       " 0.9024565982589846,\n",
       " 0.9021581267323504,\n",
       " 0.9018627394402501,\n",
       " 0.9015704030274048,\n",
       " 0.9012810845122001,\n",
       " 0.9009947512823784,\n",
       " 0.9007113710907789,\n",
       " 0.9004309120511326,\n",
       " 0.9001533426339048,\n",
       " 0.8998786316621898,\n",
       " 0.8996067483076541,\n",
       " 0.8993376620865258,\n",
       " 0.8990713428556352,\n",
       " 0.8988077608084996,\n",
       " 0.8985468864714569,\n",
       " 0.8982886906998419,\n",
       " 0.8980331446742109,\n",
       " 0.8977802198966094,\n",
       " 0.8975298881868828,\n",
       " 0.897282121679033,\n",
       " 0.8970368928176158,\n",
       " 0.8967941743541812,\n",
       " 0.8965539393437562,\n",
       " 0.8963161611413671,\n",
       " 0.8960808133986051,\n",
       " 0.8958478700602296,\n",
       " 0.8956173053608124,\n",
       " 0.8953890938214221,\n",
       " 0.8951632102463449,\n",
       " 0.8949396297198461,\n",
       " 0.8947183276029678,\n",
       " 0.8944992795303648,\n",
       " 0.8942824614071774,\n",
       " 0.8940678494059393,\n",
       " 0.8938554199635249,\n",
       " 0.893645149778128,\n",
       " 0.8934370158062787,\n",
       " 0.8932309952598934,\n",
       " 0.8930270656033614,\n",
       " 0.8928252045506617,\n",
       " 0.8926253900625174,\n",
       " 0.8924276003435803,\n",
       " 0.8922318138396501,\n",
       " 0.8920380092349244,\n",
       " 0.8918461654492824,\n",
       " 0.8916562616355992,\n",
       " 0.8914682771770905,\n",
       " 0.8912821916846909,\n",
       " 0.8910979849944594,\n",
       " 0.8909156371650175,\n",
       " 0.8907351284750163,\n",
       " 0.8905564394206322,\n",
       " 0.8903795507130938,\n",
       " 0.890204443276235,\n",
       " 0.8900310982440787,\n",
       " 0.8898594969584468,\n",
       " 0.889689620966601,\n",
       " 0.8895214520189053,\n",
       " 0.889354972066523,\n",
       " 0.8891901632591345,\n",
       " 0.8890270079426841,\n",
       " 0.8888654886571534,\n",
       " 0.88870558813436,\n",
       " 0.8885472892957799,\n",
       " 0.8883905752503992,\n",
       " 0.8882354292925871,\n",
       " 0.8880818348999957,\n",
       " 0.8879297757314826,\n",
       " 0.8877792356250593,\n",
       " 0.8876301985958623,\n",
       " 0.8874826488341478,\n",
       " 0.8873365707033104,\n",
       " 0.8871919487379236,\n",
       " 0.8870487676418046,\n",
       " 0.8869070122861,\n",
       " 0.886766667707394,\n",
       " 0.8866277191058402,\n",
       " 0.8864901518433128,\n",
       " 0.8863539514415805,\n",
       " 0.8862191035805007,\n",
       " 0.8860855940962358,\n",
       " 0.8859534089794897,\n",
       " 0.8858225343737646,\n",
       " 0.8856929565736364,\n",
       " 0.8855646620230543,\n",
       " 0.885437637313655,\n",
       " 0.8853118691831006,\n",
       " 0.8851873445134328,\n",
       " 0.8850640503294481,\n",
       " 0.8849419737970902,\n",
       " 0.8848211022218632,\n",
       " 0.8847014230472602,\n",
       " 0.8845829238532126,\n",
       " 0.8844655923545561,\n",
       " 0.8843494163995149,\n",
       " 0.8842343839682032,\n",
       " 0.8841204831711436,\n",
       " 0.8840077022478032,\n",
       " 0.883896029565147,\n",
       " 0.8837854536162071,\n",
       " 0.8836759630186681,\n",
       " 0.8835675465134712,\n",
       " 0.8834601929634305,\n",
       " 0.8833538913518689,\n",
       " 0.8832486307812686,\n",
       " 0.8831444004719354,\n",
       " 0.8830411897606811,\n",
       " 0.8829389880995195,\n",
       " 0.882837785054377,\n",
       " 0.8827375703038204,\n",
       " 0.8826383336377961,\n",
       " 0.8825400649563864,\n",
       " 0.8824427542685787,\n",
       " 0.8823463916910501,\n",
       " 0.8822509674469645,\n",
       " 0.8821564718647841,\n",
       " 0.8820628953770955,\n",
       " 0.8819702285194477,\n",
       " 0.8818784619292057,\n",
       " 0.8817875863444137,\n",
       " 0.8816975926026746,\n",
       " 0.8816084716400421,\n",
       " 0.881520214489923,\n",
       " 0.8814328122819952,\n",
       " 0.8813462562411353,\n",
       " 0.8812605376863614,\n",
       " 0.8811756480297853,\n",
       " 0.8810915787755784,\n",
       " 0.8810083215189488,\n",
       " 0.8809258679451308,\n",
       " 0.8808442098283842,\n",
       " 0.8807633390310077,\n",
       " 0.8806832475023614,\n",
       " 0.8806039272779015,\n",
       " 0.8805253704782259,\n",
       " 0.8804475693081298,\n",
       " 0.8803705160556747,\n",
       " 0.8802942030912642,\n",
       " 0.8802186228667341,\n",
       " 0.8801437679144499,\n",
       " 0.8800696308464169,\n",
       " 0.8799962043533989,\n",
       " 0.8799234812040481,\n",
       " 0.8798514542440429,\n",
       " 0.8797801163952395,\n",
       " 0.8797094606548278,\n",
       " 0.8796394800945021,\n",
       " 0.8795701678596375,\n",
       " 0.8795015171684777,\n",
       " 0.8794335213113309,\n",
       " 0.8793661736497769,\n",
       " 0.8792994676158792,\n",
       " 0.8792333967114108,\n",
       " 0.8791679545070856,\n",
       " 0.8791031346417995,\n",
       " 0.8790389308218804,\n",
       " 0.8789753368203466,\n",
       " 0.8789123464761728,\n",
       " 0.8788499536935657,\n",
       " 0.8787881524412471,\n",
       " 0.878726936751745,\n",
       " 0.8786663007206936,\n",
       " 0.8786062385061403,\n",
       " 0.87854674432786,\n",
       " 0.8784878124666796,\n",
       " 0.8784294372638081,\n",
       " 0.8783716131201738,\n",
       " 0.8783143344957713,\n",
       " 0.8782575959090142,\n",
       " 0.8782013919360946,\n",
       " 0.8781457172103524,\n",
       " 0.8780905664216482,\n",
       " 0.8780359343157471,\n",
       " 0.8779818156937058,\n",
       " 0.8779282054112694,\n",
       " 0.8778750983782733,\n",
       " 0.8778224895580529,\n",
       " 0.8777703739668599,\n",
       " 0.8777187466732835,\n",
       " 0.8776676027976809,\n",
       " 0.8776169375116122,\n",
       " 0.8775667460372817,\n",
       " 0.8775170236469872,\n",
       " 0.8774677656625738,\n",
       " 0.8774189674548931,\n",
       " 0.8773706244432722,\n",
       " 0.8773227320949839,\n",
       " 0.8772752859247273,\n",
       " 0.8772282814941101,\n",
       " 0.87718171441114,\n",
       " 0.8771355803297205,\n",
       " 0.8770898749491517,\n",
       " 0.877044594013638,\n",
       " 0.8769997333118005,\n",
       " 0.8769552886761955,\n",
       " 0.876911255982837,\n",
       " 0.8768676311507269,\n",
       " 0.8768244101413881,\n",
       " 0.8767815889584041,\n",
       " 0.8767391636469635,\n",
       " 0.8766971302934101,\n",
       " 0.8766554850247967,\n",
       " 0.8766142240084449,\n",
       " 0.8765733434515102,\n",
       " 0.8765328396005506,\n",
       " 0.8764927087411021,\n",
       " 0.8764529471972555,\n",
       " 0.8764135513312425,\n",
       " 0.8763745175430221,\n",
       " 0.8763358422698748,\n",
       " 0.8762975219859994,\n",
       " 0.8762595532021145,\n",
       " 0.8762219324650663,\n",
       " 0.876184656357438,\n",
       " 0.8761477214971656,\n",
       " 0.8761111245371562,\n",
       " 0.8760748621649143,\n",
       " 0.8760389311021656,\n",
       " 0.8760033281044926,\n",
       " 0.8759680499609683,\n",
       " 0.875933093493798,\n",
       " 0.8758984555579615,\n",
       " 0.8758641330408625,\n",
       " 0.8758301228619804,\n",
       " 0.8757964219725256,\n",
       " 0.8757630273550993,\n",
       " 0.8757299360233568,\n",
       " 0.8756971450216754,\n",
       " 0.8756646514248241,\n",
       " 0.8756324523376385,\n",
       " 0.8756005448946992,\n",
       " 0.8755689262600131,\n",
       " 0.8755375936266985,\n",
       " 0.8755065442166734,\n",
       " 0.8754757752803493,\n",
       " 0.8754452840963238,\n",
       " 0.8754150679710825,\n",
       " 0.875385124238699,\n",
       " 0.875355450260541,\n",
       " 0.8753260434249795,\n",
       " 0.8752969011471002,\n",
       " 0.8752680208684191,\n",
       " 0.8752394000566,\n",
       " 0.8752110362051764,\n",
       " 0.8751829268332761,\n",
       " 0.8751550694853477,\n",
       " 0.8751274617308932,\n",
       " 0.8751001011641986,\n",
       " 0.8750729854040722,\n",
       " 0.875046112093584,\n",
       " 0.8750194788998061,\n",
       " 0.8749930835135593,\n",
       " 0.874966923649161,\n",
       " 0.8749409970441739,\n",
       " 0.8749153014591615,\n",
       " 0.8748898346774429,\n",
       " 0.8748645945048514,\n",
       " 0.8748395787694966,\n",
       " 0.8748147853215277,\n",
       " 0.8747902120328997,\n",
       " 0.874765856797144,\n",
       " 0.8747417175291382,\n",
       " 0.8747177921648817,\n",
       " 0.8746940786612708,\n",
       " 0.8746705749958796,\n",
       " 0.8746472791667401,\n",
       " 0.874624189192126,\n",
       " 0.8746013031103399,\n",
       " 0.8745786189795011,\n",
       " 0.8745561348773366,\n",
       " 0.8745338489009742,\n",
       " 0.8745117591667386,\n",
       " 0.8744898638099478,\n",
       " 0.8744681609847139,\n",
       " 0.8744466488637448,\n",
       " 0.8744253256381487,\n",
       " 0.8744041895172394,\n",
       " 0.8743832387283466,\n",
       " 0.8743624715166247,\n",
       " 0.8743418861448659,\n",
       " 0.8743214808933155,\n",
       " 0.874301254059487,\n",
       " 0.874281203957983,\n",
       " 0.8742613289203138,\n",
       " 0.8742416272947209,\n",
       " 0.8742220974460013,\n",
       " 0.8742027377553339,\n",
       " 0.8741835466201076,\n",
       " 0.8741645224537522,\n",
       " 0.8741456636855693,\n",
       " 0.874126968760567,\n",
       " 0.874108436139295,\n",
       " 0.8740900642976822,\n",
       " 0.8740718517268757,\n",
       " 0.8740537969330824,\n",
       " 0.8740358984374105,\n",
       " 0.8740181547757144,\n",
       " 0.8740005644984411,\n",
       " 0.8739831261704774,\n",
       " 0.8739658383709995,\n",
       " 0.8739486996933234,\n",
       " 0.8739317087447579,\n",
       " 0.8739148641464596,\n",
       " 0.8738981645332863,\n",
       " 0.8738816085536573,\n",
       " 0.8738651948694096,\n",
       " 0.8738489221556601,\n",
       " 0.8738327891006665,\n",
       " 0.8738167944056912,\n",
       " 0.8738009367848649,\n",
       " 0.8737852149650551,\n",
       " 0.8737696276857321,\n",
       " 0.873754173698838,\n",
       " 0.8737388517686587,\n",
       " 0.8737236606716949,\n",
       " 0.8737085991965353,\n",
       " 0.8736936661437313,\n",
       " 0.8736788603256747,\n",
       " 0.8736641805664718,\n",
       " 0.8736496257018251,\n",
       " 0.8736351945789119,\n",
       " 0.8736208860562664,\n",
       " 0.8736066990036608,\n",
       " 0.8735926323019905,\n",
       " 0.8735786848431586,\n",
       " 0.8735648555299627,\n",
       " 0.8735511432759813,\n",
       " 0.8735375470054638,\n",
       " 0.8735240656532196,\n",
       " 0.8735106981645093,\n",
       " 0.8734974434949377,\n",
       " 0.8734843006103457,\n",
       " 0.8734712684867068,\n",
       " 0.8734583461100206,\n",
       " 0.8734455324762114,\n",
       " 0.8734328265910254,\n",
       " 0.8734202274699295,\n",
       " 0.8734077341380122,\n",
       " 0.873395345629883,\n",
       " 0.8733830609895762,\n",
       " 0.8733708792704543,\n",
       " 0.8733587995351104,\n",
       " 0.8733468208552756,\n",
       " 0.873334942311724,\n",
       " 0.87332316299418,\n",
       " 0.8733114820012277,\n",
       " 0.8732998984402187,\n",
       " 0.8732884114271835,\n",
       " 0.8732770200867419,\n",
       " 0.8732657235520156,\n",
       " 0.8732545209645407,\n",
       " 0.8732434114741823,\n",
       " 0.8732323942390493,\n",
       " 0.8732214684254097,\n",
       " 0.8732106332076075,\n",
       " 0.8731998877679814,\n",
       " 0.8731892312967805,\n",
       " 0.873178662992087,\n",
       " 0.8731681820597339,\n",
       " 0.8731577877132272,\n",
       " 0.8731474791736671,\n",
       " 0.873137255669671,\n",
       " 0.8731271164372971,\n",
       " 0.8731170607199685,\n",
       " 0.8731070877683991,\n",
       " 0.8730971968405175,\n",
       " 0.8730873872013958,\n",
       " 0.8730776581231764,\n",
       " 0.8730680088849995,\n",
       " 0.8730584387729331,\n",
       " 0.8730489470799025,\n",
       " 0.8730395331056198,\n",
       " 0.8730301961565164,\n",
       " 0.8730209355456747,\n",
       " 0.8730117505927599,\n",
       " 0.8730026406239544,\n",
       " 0.8729936049718907,\n",
       " 0.8729846429755882,\n",
       " 0.8729757539803862,\n",
       " 0.8729669373378812,\n",
       " 0.8729581924058644,\n",
       " 0.8729495185482575,\n",
       " 0.8729409151350526,\n",
       " 0.8729323815422495,\n",
       " 0.8729239171517962,\n",
       " 0.8729155213515277,\n",
       " 0.8729071935351086,\n",
       " 0.8728989331019724,\n",
       " 0.8728907394572645,\n",
       " 0.8728826120117846,\n",
       " 0.8728745501819302,\n",
       " 0.8728665533896389,\n",
       " 0.8728586210623346,\n",
       " 0.872850752632871,\n",
       " 0.872842947539478,\n",
       " 0.8728352052257066,\n",
       " 0.8728275251403772,\n",
       " 0.8728199067375257,\n",
       " 0.8728123494763503,\n",
       " 0.8728048528211623,\n",
       " 0.872797416241332,\n",
       " 0.8727900392112411,\n",
       " 0.8727827212102295,\n",
       " 0.8727754617225482,\n",
       " 0.8727682602373084,\n",
       " 0.8727611162484342,\n",
       " 0.8727540292546141,\n",
       " 0.8727469987592533,\n",
       " 0.8727400242704264,\n",
       " 0.8727331053008316,\n",
       " 0.8727262413677442,\n",
       " 0.8727194319929705,\n",
       " 0.8727126767028035,\n",
       " 0.8727059750279775,\n",
       " 0.872699326503625,\n",
       " 0.8726927306692309,\n",
       " 0.8726861870685922,\n",
       " 0.8726796952497722,\n",
       " 0.8726732547650592,\n",
       " 0.8726668651709262,\n",
       " 0.8726605260279864,\n",
       " 0.8726542369009544,\n",
       " 0.8726479973586049,\n",
       " 0.8726418069737316,\n",
       " 0.8726356653231087,\n",
       " 0.8726295719874511,\n",
       " 0.8726235265513742,\n",
       " 0.8726175286033573,\n",
       " 0.8726115777357042,\n",
       " 0.8726056735445049,\n",
       " 0.8725998156296,\n",
       " 0.8725940035945415,\n",
       " 0.8725882370465581,\n",
       " 0.8725825155965171,\n",
       " 0.8725768388588901,\n",
       " 0.8725712064517173,\n",
       " 0.8725656179965704,\n",
       " 0.872560073118521,\n",
       " 0.872554571446103,\n",
       " 0.8725491126112814,\n",
       " 0.8725436962494159,\n",
       " 0.8725383219992292,\n",
       " 0.8725329895027741,\n",
       " 0.8725276984053996,\n",
       " 0.8725224483557189,\n",
       " 0.8725172390055788,\n",
       " 0.8725120700100262,\n",
       " 0.872506941027277,\n",
       " 0.8725018517186861,\n",
       " 0.8724968017487162,\n",
       " 0.8724917907849062,\n",
       " 0.8724868184978433,\n",
       " 0.8724818845611308,\n",
       " 0.8724769886513615,\n",
       " 0.8724721304480846,\n",
       " 0.8724673096337812,\n",
       " 0.8724625258938323,\n",
       " 0.8724577789164922,\n",
       " 0.8724530683928601,\n",
       " 0.8724483940168517,\n",
       " 0.8724437554851737,\n",
       " 0.8724391524972941,\n",
       " 0.8724345847554172,\n",
       " 0.8724300519644564,\n",
       " 0.8724255538320076,\n",
       " 0.8724210900683229,\n",
       " 0.8724166603862866,\n",
       " 0.8724122645013865,\n",
       " 0.8724079021316925,\n",
       " 0.8724035729978278,\n",
       " 0.872399276822947,\n",
       " 0.8723950133327099,\n",
       " 0.8723907822552579,\n",
       " 0.8723865833211901,\n",
       " 0.8723824162635389,\n",
       " 0.8723782808177476,\n",
       " 0.8723741767216456,\n",
       " 0.8723701037154268,\n",
       " 0.8723660615416258,\n",
       " 0.8723620499450956,\n",
       " 0.8723580686729857,\n",
       " 0.872354117474719,\n",
       " 0.8723501961019708,\n",
       " 0.8723463043086471,\n",
       " 0.8723424418508623,\n",
       " 0.8723386084869182,\n",
       " 0.8723348039772839,\n",
       " 0.8723310280845744,\n",
       " 0.8723272805735293,\n",
       " 0.8723235612109935,\n",
       " 0.8723198697658959,\n",
       " 0.8723162060092309,\n",
       " 0.8723125697140374,\n",
       " 0.8723089606553789,\n",
       " 0.8723053786103261,\n",
       " 0.872301823357935,\n",
       " 0.8722982946792299,\n",
       " 0.8722947923571847,\n",
       " 0.8722913161767021,\n",
       " 0.8722878659245975,\n",
       " 0.8722844413895801,\n",
       " 0.8722810423622336,\n",
       " 0.8722776686350004,\n",
       " 0.8722743200021627,\n",
       " 0.872270996259824,\n",
       " 0.8722676972058947,\n",
       " 0.872264422640071,\n",
       " 0.8722611723638218,\n",
       " 0.8722579461803692,\n",
       " 0.8722547438946728,\n",
       " 0.872251565313413,\n",
       " 0.8722484102449753,\n",
       " 0.8722452784994332,\n",
       " 0.8722421698885325,\n",
       " 0.8722390842256759,\n",
       " 0.8722360213259069,\n",
       " 0.8722329810058943,\n",
       " 0.8722299630839172,\n",
       " 0.8722269673798488,\n",
       " 0.8722239937151428,\n",
       " 0.8722210419128171,\n",
       " 0.8722181117974404,\n",
       " 0.8722152031951158,\n",
       " 0.8722123159334682,\n",
       " 0.8722094498416286,\n",
       " 0.8722066047502206,\n",
       " 0.8722037804913466,\n",
       " 0.8722009768985727,\n",
       " 0.8721981938069164,\n",
       " 0.8721954310528318,\n",
       " 0.8721926884741973,\n",
       " 0.8721899659103004,\n",
       " 0.8721872632018268,\n",
       " 0.8721845801908457,\n",
       " 0.8721819167207967,\n",
       " 0.8721792726364785,\n",
       " 0.8721766477840345,\n",
       " 0.872174042010941,\n",
       " 0.8721714551659954,\n",
       " 0.8721688870993016,\n",
       " 0.8721663376622613,\n",
       " 0.8721638067075583,\n",
       " 0.8721612940891489,\n",
       " 0.8721587996622496,\n",
       " 0.8721563232833243,\n",
       " 0.8721538648100738,\n",
       " 0.8721514241014243,\n",
       " 0.872149001017515,\n",
       " 0.8721465954196882,\n",
       " 0.8721442071704768,\n",
       " 0.8721418361335941,\n",
       " 0.8721394821739228,\n",
       " 0.8721371451575035,\n",
       " 0.8721348249515251,\n",
       " 0.8721325214243129,\n",
       " 0.8721302344453187,\n",
       " 0.8721279638851108,\n",
       " 0.8721257096153631,\n",
       " 0.8721234715088447,\n",
       " 0.8721212494394104,\n",
       " 0.8721190432819902,\n",
       " 0.87211685291258,\n",
       " 0.872114678208231,\n",
       " 0.87211251904704,\n",
       " 0.872110375308141,\n",
       " 0.8721082468716941,\n",
       " 0.8721061336188765,\n",
       " 0.8721040354318741,\n",
       " 0.8721019521938711,\n",
       " 0.8720998837890415,\n",
       " 0.8720978301025393,\n",
       " 0.8720957910204904,\n",
       " 0.8720937664299832,\n",
       " 0.8720917562190592,\n",
       " 0.8720897602767065,\n",
       " 0.8720877784928481,\n",
       " 0.8720858107583355,\n",
       " 0.8720838569649392,\n",
       " 0.8720819170053412,\n",
       " 0.8720799907731256,\n",
       " 0.8720780781627713,\n",
       " 0.8720761790696435,\n",
       " 0.8720742933899852,\n",
       " 0.8720724210209099,\n",
       " 0.872070561860393,\n",
       " 0.8720687158072655,\n",
       " 0.8720668827612039,\n",
       " 0.8720650626227238,\n",
       " 0.8720632552931726,\n",
       " 0.8720614606747216,\n",
       " 0.8720596786703583,\n",
       " 0.8720579091838793,\n",
       " 0.8720561521198823,\n",
       " 0.8720544073837605,\n",
       " 0.8720526748816935,\n",
       " 0.8720509545206412,\n",
       " 0.8720492462083366,\n",
       " 0.8720475498532789,\n",
       " 0.8720458653647265,\n",
       " 0.8720441926526901,\n",
       " 0.8720425316279259,\n",
       " 0.8720408822019292,\n",
       " 0.8720392442869269,\n",
       " 0.8720376177958729,\n",
       " 0.8720360026424384,\n",
       " 0.8720343987410086,\n",
       " 0.8720328060066751,\n",
       " 0.8720312243552281,\n",
       " 0.8720296537031524,\n",
       " 0.8720280939676206,\n",
       " 0.8720265450664859,\n",
       " 0.872025006918277,\n",
       " 0.8720234794421918,\n",
       " 0.8720219625580913,\n",
       " 0.8720204561864937,\n",
       " 0.872018960248569,\n",
       " 0.8720174746661323,\n",
       " 0.872015999361639,\n",
       " 0.8720145342581784,\n",
       " 0.8720130792794684,\n",
       " 0.8720116343498499,\n",
       " 0.8720101993942813,\n",
       " 0.8720087743383322,\n",
       " 0.8720073591081798,\n",
       " 0.8720059536306017,\n",
       " 0.872004557832971,\n",
       " 0.8720031716432521,\n",
       " 0.8720017949899944,\n",
       " 0.872000427802327,\n",
       " 0.8719990700099546,\n",
       " 0.8719977215431509,\n",
       " 0.8719963823327557,\n",
       " 0.8719950523101674,\n",
       " 0.87199373140734,\n",
       " 0.8719924195567781,\n",
       " 0.8719911166915298,\n",
       " 0.8719898227451855,\n",
       " 0.8719885376518707,\n",
       " 0.871987261346241,\n",
       " 0.8719859937634791,\n",
       " 0.8719847348392891,\n",
       " 0.8719834845098922,\n",
       " 0.8719822427120227,\n",
       " 0.8719810093829219,\n",
       " 0.8719797844603359,\n",
       " 0.871978567882509,\n",
       " 0.8719773595881813,\n",
       " 0.8719761595165827,\n",
       " 0.8719749676074293,\n",
       " 0.87197378380092,\n",
       " 0.871972608037731,\n",
       " 0.8719714402590123,\n",
       " 0.8719702804063832,\n",
       " 0.8719691284219283,\n",
       " 0.8719679842481943,\n",
       " 0.8719668478281848,\n",
       " 0.8719657191053569,\n",
       " 0.8719645980236166,\n",
       " 0.8719634845273166,\n",
       " 0.8719623785612511,\n",
       " 0.8719612800706513,\n",
       " 0.8719601890011834,\n",
       " 0.8719591052989435,\n",
       " 0.8719580289104545,\n",
       " 0.8719569597826623,\n",
       " 0.8719558978629324,\n",
       " 0.8719548430990451,\n",
       " 0.8719537954391937,\n",
       " 0.8719527548319799,\n",
       " 0.8719517212264096,\n",
       " 0.8719506945718917,\n",
       " 0.871949674818232,\n",
       " 0.8719486619156319,\n",
       " 0.8719476558146833,\n",
       " 0.8719466564663666,\n",
       " 0.8719456638220462,\n",
       " 0.8719446778334687,\n",
       " 0.8719436984527583,\n",
       " 0.871942725632414,\n",
       " 0.8719417593253065,\n",
       " 0.8719407994846751,\n",
       " 0.8719398460641241,\n",
       " 0.8719388990176205,\n",
       " 0.8719379582994896,\n",
       " 0.8719370238644136,\n",
       " 0.8719360956674276,\n",
       " 0.8719351736639158,\n",
       " 0.8719342578096108,\n",
       " 0.8719333480605883,\n",
       " 0.8719324443732657,\n",
       " 0.8719315467043984,\n",
       " 0.8719306550110774,\n",
       " 0.8719297692507265,\n",
       " 0.8719288893810982,\n",
       " 0.8719280153602735,\n",
       " 0.8719271471466566,\n",
       " 0.8719262846989736,\n",
       " 0.8719254279762689,\n",
       " 0.8719245769379037,\n",
       " 0.8719237315435515,\n",
       " 0.8719228917531981,\n",
       " 0.871922057527136,\n",
       " 0.8719212288259645,\n",
       " 0.8719204056105843,\n",
       " 0.8719195878421986,\n",
       " 0.8719187754823068,\n",
       " 0.8719179684927045,\n",
       " 0.87191716683548,\n",
       " 0.871916370473013,\n",
       " 0.8719155793679697,\n",
       " 0.8719147934833031,\n",
       " 0.8719140127822493,\n",
       " 0.8719132372283248,\n",
       " 0.8719124667853255,\n",
       " 0.871911701417323,\n",
       " 0.8719109410886624,\n",
       " 0.8719101857639618,\n",
       " 0.8719094354081075,\n",
       " 0.871908689986253,\n",
       " 0.8719079494638178,\n",
       " 0.8719072138064831,\n",
       " 0.8719064829801908,\n",
       " 0.8719057569511419,\n",
       " 0.8719050356857927,\n",
       " 0.8719043191508544,\n",
       " 0.87190360731329,\n",
       " 0.8719029001403118,\n",
       " 0.8719021975993815,\n",
       " 0.8719014996582051,\n",
       " 0.8719008062847333,\n",
       " 0.8719001174471583,\n",
       " 0.871899433113912,\n",
       " 0.8718987532536647,\n",
       " 0.8718980778353217,\n",
       " 0.8718974068280231,\n",
       " 0.8718967402011405,\n",
       " 0.8718960779242757,\n",
       " 0.8718954199672586,\n",
       " 0.8718947663001463,\n",
       " 0.8718941168932193,\n",
       " 0.8718934717169813,\n",
       " 0.8718928307421567,\n",
       " 0.8718921939396892,\n",
       " 0.8718915612807392,\n",
       " 0.8718909327366833,\n",
       " 0.8718903082791114,\n",
       " 0.8718896878798255,\n",
       " 0.8718890715108377,\n",
       " 0.8718884591443687,\n",
       " 0.8718878507528462,\n",
       " 0.8718872463089028,\n",
       " 0.871886645785375,\n",
       " 0.8718860491553008,\n",
       " 0.8718854563919183,\n",
       " 0.8718848674686648,\n",
       " 0.8718842823591739,\n",
       " 0.871883701037275,\n",
       " 0.871883123476991,\n",
       " 0.8718825496525376,\n",
       " 0.8718819795383208,\n",
       " 0.8718814131089356,\n",
       " 0.8718808503391646,\n",
       " 0.8718802912039771,\n",
       " 0.8718797356785265,\n",
       " 0.8718791837381494,\n",
       " 0.8718786353583644,\n",
       " 0.8718780905148698,\n",
       " 0.8718775491835431,\n",
       " 0.8718770113404384,\n",
       " 0.8718764769617866,\n",
       " 0.8718759460239929,\n",
       " 0.8718754185036353,\n",
       " 0.8718748943774635,\n",
       " 0.8718743736223975,\n",
       " 0.8718738562155265,\n",
       " 0.8718733421341076,\n",
       " 0.8718728313555635,\n",
       " 0.8718723238574821,\n",
       " 0.8718718196176157,\n",
       " 0.8718713186138777,\n",
       " 0.8718708208243436,\n",
       " 0.8718703262272478,\n",
       " 0.8718698348009841,\n",
       " 0.871869346524103,\n",
       " 0.8718688613753109,\n",
       " 0.8718683793334696,\n",
       " 0.8718679003775938,\n",
       " 0.871867424486851,\n",
       " 0.871866951640559,\n",
       " 0.871866481818187,\n",
       " 0.8718660149993518,\n",
       " 0.8718655511638178,\n",
       " 0.8718650902914962,\n",
       " 0.8718646323624434,\n",
       " 0.87186417735686,\n",
       " 0.8718637252550889,\n",
       " 0.8718632760376162,\n",
       " 0.8718628296850671,\n",
       " 0.8718623861782079,\n",
       " 0.8718619454979426,\n",
       " 0.8718615076253127,\n",
       " 0.8718610725414966,\n",
       " 0.8718606402278075,\n",
       " 0.8718602106656929,\n",
       " 0.8718597838367337,\n",
       " 0.8718593597226427,\n",
       " 0.8718589383052646,\n",
       " 0.8718585195665731,\n",
       " 0.871858103488672,\n",
       " 0.8718576900537919,\n",
       " 0.8718572792442922,\n",
       " 0.8718568710426573,\n",
       " 0.8718564654314969,\n",
       " 0.8718560623935445,\n",
       " 0.8718556619116576,\n",
       " 0.8718552639688157,\n",
       " 0.8718548685481192]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from SoftmaxClassifier import SoftmaxClassifier\n",
    "import numpy as np\n",
    "\n",
    "softmax = SoftmaxClassifier(alpha=0.1)\n",
    "x = np.random.rand(3,3)\n",
    "y = np.array([0, 1, 2])\n",
    "softmax.fit(x, y).losses_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Pour limiter l'**overfitting**, on utilise la régularisation, il s'agit d'ajouter un terme à la fonction de coût $J( \\Theta)$.\n",
    "\n",
    "Ce terme va ajouter des contraintes sur les poids du modèle lors de l'entrainement.\n",
    "Nous allons utiliser la régularisation **L2** :\n",
    "\n",
    "$$ L2(\\Theta) = \\alpha \\sum_{\\substack{1<=i<n}} \\sum_{\\substack{0<=k<K}} \\theta_{i,k}^2 $$ \n",
    "\n",
    "avec:\n",
    "\n",
    "* $\\alpha$ le coefficient de régularisation\n",
    "\n",
    "**Remarque:** La première somme ne commence pas à 0 mais à 1 parce qu'on ne régularise pas les poids associés à la colonne de biais de X.\n",
    "\n",
    "Le fait d'ajouter ce terme conduit le modèle à apprendre les données tout en gardant ses poids le plus petit possible.\n",
    "\n",
    "\n",
    "\n",
    "#### Question 8 (1 point)\n",
    "Modifiez les fonctions  **_get_gradient** et **_cost_function** pour prendre en compte la régularisation lorsque le booléen self.regularization est vrai  dans SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4747839579749682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.14743648,  0.04825759,  0.09917889],\n",
       "       [ 0.06784513,  0.04740148, -0.11547569],\n",
       "       [-0.0382107 ,  0.13579189, -0.09807479],\n",
       "       [-0.13081621,  0.06579134,  0.06563377]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.rand(3,3)\n",
    "y = np.array([0, 1, 2])\n",
    "predict_proba = softmax.predict_proba(x)\n",
    "softmax.predict(x)\n",
    "print(softmax._cost_function(predict_proba, y))\n",
    "X_bias = np.concatenate((x, np.ones((x.shape[0], 1))), axis=1)\n",
    "softmax._get_gradient(X_bias, y, predict_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9 (1 point)\n",
    "\n",
    "Le terme de régularisation est utilisé uniquement pendant l'entraînement. Quand on veut évaluer la performance du modèle **après entrainement**, on utilise la fonction de coût **non-régulée**.\n",
    "\n",
    "Implémentez la fonction **score** qui permet d'évaluer la qualité de la prédiction **après entrainement** dans SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3345909483545348"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping\n",
    "\n",
    "Un trop grand nombre d'**epoch** peut résulter en **overfitting**.\n",
    "Pour pallier à ce problème, on peut utiliser le mécanisme d'**early stopping**.\n",
    "Il s'agit d'arrêter l'entraînement si la différence de la fonction de coût entre deux **epochs consécutives** est inférieure à un **seuil**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Question 10 (1 point)\n",
    "\n",
    "Finissez d'implémenter la fonction **fit** en y ajoutant le mécanisme d'**early stopping**  quand le booléen **self.early_stopping** est vrai le seuil est donné par la variable **self.threshold**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.145719000063237, 1.1401828257202968]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from SoftmaxClassifier import SoftmaxClassifier\n",
    "import numpy as np\n",
    "\n",
    "softmax = SoftmaxClassifier(alpha=0.1, early_stopping=True, threshold=0.01)\n",
    "x = np.random.rand(3,3)\n",
    "y = np.array([0, 1, 2])\n",
    "softmax.fit(x, y).losses_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de la solution:\n",
    "\n",
    "Le code ci-dessous importe le dataset de classification multiclasse **iris** disponible sur sklearn. Les données sont divisées en deux parties, l'ensemble d'entraînement et l'ensemble de test, puis elles sont normalisées.\n",
    "\n",
    "Le classifier implémenté dans le fichier **SoftmaxClassifier.py** est importé puis entrainé sur l'ensemble d'entrainement et testé sur l'ensemble de test.\n",
    "\n",
    "Le but de cette partie est juste de vérifier votre implémentation **quand vous êtes sûrs que votre code fonctionne**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# load dataset\n",
    "data,target =load_iris().data,load_iris().target\n",
    "\n",
    "# split data in train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split( data, target, test_size=0.33, random_state=42)\n",
    "\n",
    "# standardize columns using normal distribution\n",
    "# fit on X_train and not on X_test to avoid Data Leakage\n",
    "s = StandardScaler()\n",
    "X_train = s.fit_transform(X_train)\n",
    "X_test = s.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SoftmaxClassifier import SoftmaxClassifier\n",
    "\n",
    "# import the custom classifier\n",
    "cl = SoftmaxClassifier(alpha=0.1, regularization = True, use_zero_indexed_classes = True)\n",
    "\n",
    "# train on X_train and not on X_test to avoid overfitting\n",
    "train_p = cl.fit_predict(X_train,y_train)\n",
    "test_p = cl.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous obtenez des valeurs relativement proches pour l'ensemble de test et d'entrainement, et qu'elles sont au moins supérieures à 0.8, votre modèle devrait être correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : (0.9729729729729729, 0.9714285714285714, 0.9709901198234182, None)\n",
      "test : (0.9791666666666666, 0.9791666666666666, 0.978494623655914, None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# display precision, recall and f1-score on train/test set\n",
    "print(\"train : \"+ str(precision_recall_fscore_support(y_train, train_p,average = \"macro\")))\n",
    "print(\"test : \"+ str(precision_recall_fscore_support(y_test, test_p,average = \"macro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(cl.losses_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data preprocessing (8 points)\n",
    "\n",
    "##  Kaggle \n",
    "Kaggle est un site dédié au machine learning. On y retrouve un grand nombre de dataset.\n",
    "Des compétitions sont organisées par des organisations. Ces dernières fournissent un dataset et un objectif. Les \"kagglers\" qui participent à ces compétitions soumettent leurs résultats en ligne. Il y a souvent des prix ou des emplois pour ceux qui obtiennent les meilleurs résultats.\n",
    "\n",
    "Il s'agit d'un bon moyen pour développer ses compétences en machine learning sur des vrais datasets.\n",
    "\n",
    "Vous pouvez créer un compte si vous voulez comparer vos résultats à ceux déjà en ligne pour la dataset que nous allons étudier.\n",
    "\n",
    "Vous pouvez créer un compte ici: https://www.kaggle.com/\n",
    "\n",
    "\n",
    "## Austin Animal Center Shelter Animal Outcomes dataset\n",
    "Le dataset que nous utiliserons est le \"Animal Outcomes dataset\" disponible à l'adresse suivante: https://www.kaggle.com/c/shelter-animal-outcomes.\n",
    "\n",
    "Il s'agit d'un problème de **classification multiclasse** des animaux sont recueillis dans un refuge après avoir été abandonnés, le but est de prédire la manière dont ils vont \"quitter \" le lieu:\n",
    "* Adoption\n",
    "* Retour au propriétaire\n",
    "* Décès \n",
    "* Euthanasie\n",
    "* Transfert à un autre centre\n",
    "\n",
    "Pour plus d'informations sur les données, rendez-vous sur kaggle.\n",
    "\n",
    "## Déroulement d'un projet de machine learning\n",
    "\n",
    "Le but de la suite de ce TP est de vous faire étudier une version simplifiée d'un projet complet de machine learning:\n",
    "\n",
    "1. Nettoyage des données, traitement des valeurs manquantes\n",
    "2. Mise en forme des données pour pouvoir les utiliser dans les algorithmes de machine learning\n",
    "3. Feature engineering: transformation ou combinaison de features entre elles\n",
    "4. Comparaison des performances des différents choix effectués lors du traîtement des données\n",
    "5. Comparaison des performances de différents modèles (dont celui implémenté en première partie)\n",
    "6. Optimisation des hyper-paramètres\n",
    "\n",
    "## Scikit-learn\n",
    "http://scikit-learn.org/stable/\n",
    "\n",
    "Il s'agit d'une bibliothèque de machine learning et data mining, elle propose des outils pour l'analyse et le traîtement des données,  des algorithmes classiques de machine learning comme les réseaux de neuronnes, la régression logistique, les SVM ou autre, enfin des outils permettant de comparer les modèles entre eux comme la cross validation.\n",
    "\n",
    "## Pandas\n",
    "\n",
    "Une bibliothèque permettant de stocker des données et de les manipuler facilement\n",
    "\n",
    "Les deux éléments de base de pandas sont le dataframe et la serie.\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.html\n",
    "\n",
    "## Data processing tutorial\n",
    "\n",
    "**Avant de continuer le TP**, familiarisez-vous avec le **pré-traitement des données**, **pandas** et **scikit-learn**, un tutoriel est disponible dans le fichier: **data_processing_tutorial.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "#### Chargement de l'ensemble d'entraînement et de l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PATH = \"data/\"\n",
    "X_train = pd.read_csv(PATH + \"train.csv\")\n",
    "X_test = pd.read_csv(PATH + \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppression de colonnes inutiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns = [\"OutcomeSubtype\",\"AnimalID\"])\n",
    "X_test = X_test.drop(columns = [\"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train.drop(columns = [\"OutcomeType\"]),X_train[\"OutcomeType\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 premiers exemples de l'ensemble d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>AnimalType</th>\n",
       "      <th>SexuponOutcome</th>\n",
       "      <th>AgeuponOutcome</th>\n",
       "      <th>Breed</th>\n",
       "      <th>Color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hambone</td>\n",
       "      <td>2014-02-12 18:22:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>1 year</td>\n",
       "      <td>Shetland Sheepdog Mix</td>\n",
       "      <td>Brown/White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Emily</td>\n",
       "      <td>2013-10-13 12:44:00</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Spayed Female</td>\n",
       "      <td>1 year</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>Cream Tabby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pearce</td>\n",
       "      <td>2015-01-31 12:28:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Pit Bull Mix</td>\n",
       "      <td>Blue/White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-07-11 19:09:00</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>3 weeks</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>Blue Cream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-11-15 12:52:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Lhasa Apso/Miniature Poodle</td>\n",
       "      <td>Tan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name             DateTime AnimalType SexuponOutcome AgeuponOutcome  \\\n",
       "0  Hambone  2014-02-12 18:22:00        Dog  Neutered Male         1 year   \n",
       "1    Emily  2013-10-13 12:44:00        Cat  Spayed Female         1 year   \n",
       "2   Pearce  2015-01-31 12:28:00        Dog  Neutered Male        2 years   \n",
       "3      NaN  2014-07-11 19:09:00        Cat    Intact Male        3 weeks   \n",
       "4      NaN  2013-11-15 12:52:00        Dog  Neutered Male        2 years   \n",
       "\n",
       "                         Breed        Color  \n",
       "0        Shetland Sheepdog Mix  Brown/White  \n",
       "1       Domestic Shorthair Mix  Cream Tabby  \n",
       "2                 Pit Bull Mix   Blue/White  \n",
       "3       Domestic Shorthair Mix   Blue Cream  \n",
       "4  Lhasa Apso/Miniature Poodle          Tan  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 premiers exemples de l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>AnimalType</th>\n",
       "      <th>SexuponOutcome</th>\n",
       "      <th>AgeuponOutcome</th>\n",
       "      <th>Breed</th>\n",
       "      <th>Color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summer</td>\n",
       "      <td>2015-10-12 12:15:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Intact Female</td>\n",
       "      <td>10 months</td>\n",
       "      <td>Labrador Retriever Mix</td>\n",
       "      <td>Red/White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cheyenne</td>\n",
       "      <td>2014-07-26 17:59:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Spayed Female</td>\n",
       "      <td>2 years</td>\n",
       "      <td>German Shepherd/Siberian Husky</td>\n",
       "      <td>Black/Tan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gus</td>\n",
       "      <td>2016-01-13 12:20:00</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>1 year</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>Brown Tabby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pongo</td>\n",
       "      <td>2013-12-28 18:12:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>4 months</td>\n",
       "      <td>Collie Smooth Mix</td>\n",
       "      <td>Tricolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skooter</td>\n",
       "      <td>2015-09-24 17:59:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Miniature Poodle Mix</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Name             DateTime AnimalType SexuponOutcome AgeuponOutcome  \\\n",
       "0    Summer  2015-10-12 12:15:00        Dog  Intact Female      10 months   \n",
       "1  Cheyenne  2014-07-26 17:59:00        Dog  Spayed Female        2 years   \n",
       "2       Gus  2016-01-13 12:20:00        Cat  Neutered Male         1 year   \n",
       "3     Pongo  2013-12-28 18:12:00        Dog    Intact Male       4 months   \n",
       "4   Skooter  2015-09-24 17:59:00        Dog  Neutered Male        2 years   \n",
       "\n",
       "                            Breed        Color  \n",
       "0          Labrador Retriever Mix    Red/White  \n",
       "1  German Shepherd/Siberian Husky    Black/Tan  \n",
       "2          Domestic Shorthair Mix  Brown Tabby  \n",
       "3               Collie Smooth Mix     Tricolor  \n",
       "4            Miniature Poodle Mix        White  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 premiers exemples de l'attribut à prédire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Return_to_owner\n",
       "1         Euthanasia\n",
       "2           Adoption\n",
       "3           Transfer\n",
       "4           Transfer\n",
       "Name: OutcomeType, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travail demandé\n",
    "\n",
    "Pour vous faire gagner du temps, une partie des colonnes (Name,DateTime,color) ont déjà été traitées.\n",
    "\n",
    "\n",
    "En vous appuyant sur le tutoriel fourni, vous devez écrire un pipeline complet de transformation pour chacune des colonnes restantes du dataset (AgeuponOutcome,AnimalType,SexuponOutcome, Breed).\n",
    "\n",
    "Vous êtes **libres** de vos choix, mais vous devez les **justifer** colonne par colonne.\n",
    "Par exemple, vous pouvez choisir de combiner des colonnes entre elles, de séparer une colonne en plusieurs ou encore d'éliminer complètement une colonne si vous le justifiez correctement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La partie déjà prétraitée du dataset est chargée dans **X_train1** et **X_test1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = pd.read_csv(\"data/train_preprocessed.csv\")\n",
    "X_test1 = pd.read_csv(\"data/test_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Color</th>\n",
       "      <th>HasName</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.973624</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.421532</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.973624</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.471381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.868974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Color  HasName  Month  Day  Hour\n",
       "0  0.973624      1.0    2.0  1.0   3.0\n",
       "1 -1.421532      1.0   10.0  1.0   2.0\n",
       "2  0.973624      1.0    1.0  3.0   2.0\n",
       "3 -1.471381      0.0    7.0  1.0   3.0\n",
       "4 -0.868974      0.0   11.0  1.0   2.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le reste du dataset que vous devez traiter est:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns = [\"Color\",\"Name\",\"DateTime\"])\n",
    "X_test = X_test.drop(columns = [\"Color\",\"Name\",\"DateTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AnimalType</th>\n",
       "      <th>SexuponOutcome</th>\n",
       "      <th>AgeuponOutcome</th>\n",
       "      <th>Breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>1 year</td>\n",
       "      <td>Shetland Sheepdog Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cat</td>\n",
       "      <td>Spayed Female</td>\n",
       "      <td>1 year</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Pit Bull Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cat</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>3 weeks</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Lhasa Apso/Miniature Poodle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AnimalType SexuponOutcome AgeuponOutcome                        Breed\n",
       "0        Dog  Neutered Male         1 year        Shetland Sheepdog Mix\n",
       "1        Cat  Spayed Female         1 year       Domestic Shorthair Mix\n",
       "2        Dog  Neutered Male        2 years                 Pit Bull Mix\n",
       "3        Cat    Intact Male        3 weeks       Domestic Shorthair Mix\n",
       "4        Dog  Neutered Male        2 years  Lhasa Apso/Miniature Poodle"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Question 11: AgeuponOutcome (1 point)\n",
    "\n",
    "On remplit d'abord les valeurs NaN avec la valeur textuelle la plus courante trouvée, c'est-à-dire \"1 year\". Cela nous permet ensuite d'appliquer une fonction de transformation personnalisée, qui va lire la string de forme \"X day/week/month/year(s)\" et la transformer en valeur numérique correspondant à l'âge en jours de l'animal. Finalement, on normalise ces valeurs numériques avec un *StandardScaler* pour les rendre significatives (on veut surtout observer une différence entre un animal très jeune et un animal très vieux, pas entre 5 mois et 6 mois)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AnimalType</th>\n",
       "      <th>SexuponOutcome</th>\n",
       "      <th>AgeuponOutcome</th>\n",
       "      <th>Breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dog</td>\n",
       "      <td>Intact Female</td>\n",
       "      <td>-0.453005</td>\n",
       "      <td>Labrador Retriever Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dog</td>\n",
       "      <td>Spayed Female</td>\n",
       "      <td>-0.059510</td>\n",
       "      <td>German Shepherd/Siberian Husky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cat</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>-0.396792</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dog</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>-0.621646</td>\n",
       "      <td>Collie Smooth Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>-0.059510</td>\n",
       "      <td>Miniature Poodle Mix</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AnimalType SexuponOutcome  AgeuponOutcome                           Breed\n",
       "0        Dog  Intact Female       -0.453005          Labrador Retriever Mix\n",
       "1        Dog  Spayed Female       -0.059510  German Shepherd/Siberian Husky\n",
       "2        Cat  Neutered Male       -0.396792          Domestic Shorthair Mix\n",
       "3        Dog    Intact Male       -0.621646               Collie Smooth Mix\n",
       "4        Dog  Neutered Male       -0.059510            Miniature Poodle Mix"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def parse_age(text):\n",
    "    time = 0.0\n",
    "    multiplier, timeframe = text.split(\" \")\n",
    "    multiplier = float(multiplier)\n",
    "    if \"day\" in timeframe:\n",
    "        time = multiplier\n",
    "    elif \"week\" in timeframe:\n",
    "        time = multiplier * 7.0\n",
    "    elif \"month\" in timeframe:\n",
    "        time = multiplier * (365.25/12.0)\n",
    "    elif \"year\" in timeframe:\n",
    "        time = multiplier * 365.25\n",
    "    return time\n",
    "\n",
    "# 1 year est l'entrée la plus courante selon value_counts()\n",
    "age_imputer = SimpleImputer(strategy = 'constant', fill_value = '1 year')\n",
    "age_scaler = StandardScaler()\n",
    "\n",
    "X_train[\"AgeuponOutcome\"] = age_imputer.fit_transform(X_train[\"AgeuponOutcome\"].values.reshape(-1,1))\n",
    "X_test[\"AgeuponOutcome\"] = age_imputer.transform(X_test[\"AgeuponOutcome\"].values.reshape(-1,1))\n",
    "\n",
    "X_train[\"AgeuponOutcome\"] = X_train.apply(lambda row: pd.Series(parse_age(row[\"AgeuponOutcome\"])), axis=1)\n",
    "X_test[\"AgeuponOutcome\"] = X_test.apply(lambda row: pd.Series(parse_age(row[\"AgeuponOutcome\"])), axis=1)\n",
    "\n",
    "X_train[\"AgeuponOutcome\"] = age_scaler.fit_transform(X_train[\"AgeuponOutcome\"].values.reshape(-1,1))\n",
    "X_test[\"AgeuponOutcome\"] = age_scaler.transform(X_test[\"AgeuponOutcome\"].values.reshape(-1,1))\n",
    "\n",
    "X_train.head()\n",
    "X_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12: AnimalType (1 point)\n",
    "\n",
    "On élimine la colonne AnimalType pour la transformer en une simple catégorisation Vrai/Faux pour si c'est un chien ou non. En effet, on a seulement deux types d'animaux : Chien et Chat. Faire cela nous permet d'encoder la valeur en 0 ou 1, la rendant fonctionnelle pour le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\preprocessing\\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SexuponOutcome</th>\n",
       "      <th>AgeuponOutcome</th>\n",
       "      <th>Breed</th>\n",
       "      <th>IsDog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intact Female</td>\n",
       "      <td>-0.453005</td>\n",
       "      <td>Labrador Retriever Mix</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spayed Female</td>\n",
       "      <td>-0.059510</td>\n",
       "      <td>German Shepherd/Siberian Husky</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>-0.396792</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Intact Male</td>\n",
       "      <td>-0.621646</td>\n",
       "      <td>Collie Smooth Mix</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>-0.059510</td>\n",
       "      <td>Miniature Poodle Mix</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SexuponOutcome  AgeuponOutcome                           Breed  IsDog\n",
       "0  Intact Female       -0.453005          Labrador Retriever Mix      1\n",
       "1  Spayed Female       -0.059510  German Shepherd/Siberian Husky      1\n",
       "2  Neutered Male       -0.396792          Domestic Shorthair Mix      0\n",
       "3    Intact Male       -0.621646               Collie Smooth Mix      1\n",
       "4  Neutered Male       -0.059510            Miniature Poodle Mix      1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dog_label = LabelEncoder()\n",
    "\n",
    "X_train[\"IsDog\"] = dog_label.fit_transform(X_train[\"AnimalType\"].values)\n",
    "X_test[\"IsDog\"] = dog_label.transform(X_test[\"AnimalType\"].values.reshape(-1,1))\n",
    "\n",
    "X_train = X_train.drop(columns = [\"AnimalType\"])\n",
    "X_test = X_test.drop(columns = [\"AnimalType\"])\n",
    "X_train.head()\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 13: SexuponOutcome (1 point)\n",
    "\n",
    "On remplit d'abord les valeurs NaN avec l'équivalent \"Unknown\" (ce qui veut dire que le sexe de l'animal est inconnu). Ensuite, on *parse* les entrées textuelles pour les séparer en deux *features* : l'état de l'animal (\"State\") et son sexe à proprement parler. L'état correspond à s'il peut se reproduire ou non, donc \"Spayed\" et \"Neutered\" correspondent à la même classe (pour femelle et mâle respectivement). On mettra \"Intact\" et \"Unknown\" dans leur classe respective (car on a un nombre tout de même significatif de valeurs \"Unknown\", donc on ne veut pas diluer les données en supposant qu'il s'agit de la valeur la plus courante (*Neutered Male* ici)). Ainsi, on aura des valeurs textuelles transformées en *labels* individuels *State* { Neutered, Intact, Unknown } et *Sex* { Male, Female, Unknown }. On finalise en transformant ces *labels* avec un encodeur one-hot pour obtenir 6 colonnes (3 pour le *State* et 3 pour le *Sex*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\preprocessing\\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AgeuponOutcome</th>\n",
       "      <th>Breed</th>\n",
       "      <th>IsDog</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.453005</td>\n",
       "      <td>Labrador Retriever Mix</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.059510</td>\n",
       "      <td>German Shepherd/Siberian Husky</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.396792</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.621646</td>\n",
       "      <td>Collie Smooth Mix</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.059510</td>\n",
       "      <td>Miniature Poodle Mix</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AgeuponOutcome                           Breed  IsDog    0    1    2    0  \\\n",
       "0       -0.453005          Labrador Retriever Mix      1  1.0  0.0  0.0  1.0   \n",
       "1       -0.059510  German Shepherd/Siberian Husky      1  0.0  1.0  0.0  1.0   \n",
       "2       -0.396792          Domestic Shorthair Mix      0  0.0  1.0  0.0  0.0   \n",
       "3       -0.621646               Collie Smooth Mix      1  1.0  0.0  0.0  0.0   \n",
       "4       -0.059510            Miniature Poodle Mix      1  0.0  1.0  0.0  0.0   \n",
       "\n",
       "     1    2  \n",
       "0  0.0  0.0  \n",
       "1  0.0  0.0  \n",
       "2  1.0  0.0  \n",
       "3  1.0  0.0  \n",
       "4  1.0  0.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#Custom functions\n",
    "def parse_state(text):\n",
    "    if \"Unknown\" in text:\n",
    "        return \"Unknown\"\n",
    "    state, _ = text.split(\" \")\n",
    "    if \"Spayed\" in state:\n",
    "        state = \"Neutered\"\n",
    "    return state\n",
    "\n",
    "def parse_sex(text):\n",
    "    if \"Unknown\" in text:\n",
    "        return \"Unknown\"\n",
    "    _, sex = text.split(\" \")\n",
    "    return sex\n",
    "\n",
    "# TODO: Try using two one-hot vectors to handle the Unknown type\n",
    "\n",
    "sex_imputer = SimpleImputer(strategy = 'constant', fill_value = 'Unknown')\n",
    "\n",
    "X_train[\"SexuponOutcome\"] = sex_imputer.fit_transform(X_train[\"SexuponOutcome\"].values.reshape(-1,1))\n",
    "X_test[\"SexuponOutcome\"] = sex_imputer.transform(X_test[\"SexuponOutcome\"].values.reshape(-1,1))\n",
    "\n",
    "# Train set\n",
    "state_train = X_train.apply(lambda row: pd.Series(  parse_state(row[\"SexuponOutcome\"])  ), axis = 1  )\n",
    "sex_train = X_train.apply(lambda row: pd.Series(  parse_sex(row[\"SexuponOutcome\"])  ), axis = 1  )\n",
    "\n",
    "state_train.columns = [\"State\"]\n",
    "sex_train.columns = [\"Sex\"]\n",
    "\n",
    "new_columns = pd.concat([state_train, sex_train], axis = 1)\n",
    "X_train = X_train.drop(columns = [\"SexuponOutcome\"])\n",
    "X_train = pd.concat([X_train,pd.DataFrame(new_columns)], axis = 1)\n",
    "\n",
    "# Test set\n",
    "state_test = X_test.apply(lambda row: pd.Series(  parse_state(row[\"SexuponOutcome\"])  ), axis = 1  )\n",
    "sex_test = X_test.apply(lambda row: pd.Series(  parse_sex(row[\"SexuponOutcome\"])  ), axis = 1  )\n",
    "\n",
    "state_test.columns = [\"State\"]\n",
    "sex_test.columns = [\"Sex\"]\n",
    "\n",
    "new_columns = pd.concat([state_test, sex_test], axis = 1)\n",
    "X_test = X_test.drop(columns = [\"SexuponOutcome\"])\n",
    "X_test = pd.concat([X_test,pd.DataFrame(new_columns)], axis = 1)\n",
    "\n",
    "# encoder initialisation\n",
    "state_label = LabelEncoder()\n",
    "sex_label = LabelEncoder()\n",
    "\n",
    "#Train set\n",
    "X_train[\"State\"] = state_label.fit_transform(X_train[\"State\"].values)\n",
    "X_train[\"Sex\"] = sex_label.fit_transform(X_train[\"Sex\"].values)\n",
    "\n",
    "#Test set\n",
    "X_test[\"State\"] = state_label.transform(X_test[\"State\"].values.reshape(-1,1))\n",
    "X_test[\"Sex\"] = sex_label.transform(X_test[\"Sex\"].values.reshape(-1,1))\n",
    "\n",
    "# encoder initialisation\n",
    "categorical_encoder = OneHotEncoder(categories = 'auto', sparse = False)\n",
    "\n",
    "#Train set\n",
    "new_columns_train = pd.concat([pd.DataFrame(categorical_encoder.fit_transform(X_train[\"State\"].values.reshape(-1,1))), pd.DataFrame(categorical_encoder.fit_transform(X_train[\"Sex\"].values.reshape(-1,1)))], axis = 1)\n",
    "X_train = X_train.drop(columns=[\"State\", \"Sex\"])\n",
    "X_train = pd.concat([X_train,pd.DataFrame(new_columns_train)], axis = 1)\n",
    "\n",
    "#Test set\n",
    "new_columns_test = pd.concat([pd.DataFrame(categorical_encoder.transform(X_test[\"State\"].values.reshape(-1,1))), pd.DataFrame(categorical_encoder.transform(X_test[\"Sex\"].values.reshape(-1,1)))], axis = 1)\n",
    "X_test = X_test.drop(columns=[\"State\", \"Sex\"])\n",
    "X_test = pd.concat([X_test,pd.DataFrame(new_columns_test)], axis = 1)\n",
    "\n",
    "X_train.head()\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 14: Breed (1 point)\n",
    "\n",
    "La colonne breed est problématique puisqu'elle contient un nombre élevé de classes différentes et certaines sont très rares (présentes seulement 1 fois dans le dataset). Pour pallier à ce problème, nous effectuons plusieurs manipulations afin de réduire le nombre de races possibles :\n",
    "\n",
    "1. Ajout d'une colonne indiquant si l'animal est un croisement/mix (1 si la race fini avec \"Mix\" ou contient le caractère \"/\", 0 sinon) \n",
    "2. Retrait du terme \"Mix\" des noms de race\n",
    "3. Lorsque le nom de race est un mix de la forme X/Y, nous gardons seulement la première race, soit X.\n",
    "\n",
    "Ces modifications ont permis de réduire le nombre de classes de 1381 à 225.\n",
    "\n",
    "Avec les classes résultantes, on effectue un encodage binaire. Cet encodage effectue l'équivalent d'un *LabelEncoder* pour ensuite encoder ces valeurs en binaire et obtenir des colonnes ressemblant à celles du one-hot, mais avec une dimensionnalité très réduite. On obtient ainsi 9 colonnes en plus du Mix. Ce type d'encodage est apparemment efficace avec des algorithmes d'apprentissage basés sur des arbres de décision : http://www.willmcginnis.com/2015/11/29/beyond-one-hot-an-exploration-of-categorical-variables/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders import BinaryEncoder\n",
    "\n",
    "def parse_mix(text):\n",
    "    if text.endswith(\" Mix\") or \"/\" in text:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def remove_mix(text):\n",
    "    if text.endswith(\" Mix\"):\n",
    "        return text[:-4]\n",
    "    return text\n",
    "\n",
    "def remove_slash(text):\n",
    "    index = text.find('/')\n",
    "    if index > -1:\n",
    "        return text[:index]\n",
    "    return text\n",
    "\n",
    "column = \"Breed\"\n",
    "mix_column = \"Mix\"\n",
    "breed_imputer = SimpleImputer(strategy = 'constant', fill_value = 'Unknown')\n",
    "\n",
    "# Train\n",
    "X_train[column] = breed_imputer.fit_transform(X_train[column].values.reshape(-1,1))\n",
    "\n",
    "mix_train = X_train.apply(lambda row: pd.Series(  parse_mix(row[column])  ), axis = 1 )\n",
    "mix_train.columns = [mix_column]\n",
    "\n",
    "breed_train = X_train.apply(lambda row: pd.Series(  remove_mix(row[column])  ), axis = 1 )\n",
    "breed_train.columns = [column]\n",
    "breed_train = breed_train.apply(lambda row: pd.Series(  remove_slash(row[column])  ), axis = 1 )\n",
    "breed_train.columns = [column]\n",
    "\n",
    "new_columns = pd.concat([mix_train, breed_train], axis = 1)\n",
    "\n",
    "X_train = X_train.drop(columns = [column])\n",
    "X_train = pd.concat([X_train,pd.DataFrame(new_columns)], axis = 1)\n",
    "\n",
    "# Test\n",
    "X_test[column] = breed_imputer.fit_transform(X_test[column].values.reshape(-1,1))\n",
    "\n",
    "mix_test = X_test.apply(lambda row: pd.Series(  parse_mix(row[column])  ), axis = 1 )\n",
    "mix_test.columns = [mix_column]\n",
    "\n",
    "breed_test = X_test.apply(lambda row: pd.Series(  remove_mix(row[column])  ), axis = 1 )\n",
    "breed_test.columns = [column]\n",
    "breed_test = breed_test.apply(lambda row: pd.Series(  remove_slash(row[column])  ), axis = 1 )\n",
    "breed_test.columns = [column]\n",
    "\n",
    "new_columns = pd.concat([mix_test, breed_test], axis = 1)\n",
    "\n",
    "X_test = X_test.drop(columns = [column])\n",
    "X_test = pd.concat([X_test,pd.DataFrame(new_columns)], axis = 1)\n",
    "\n",
    "breed_label = LabelEncoder()\n",
    "breed_onehot = OneHotEncoder(categories = 'auto', sparse = False, handle_unknown='ignore')\n",
    "breed_binary = BinaryEncoder(impute_missing=True, handle_unknown='impute')\n",
    "\n",
    "# Train\n",
    "X_train[column] = breed_binary.fit_transform(breed_train[column].values)\n",
    "\n",
    "\n",
    "\n",
    "new_columns_train = breed_binary.fit_transform(breed_train[column].values.reshape(-1,1))\n",
    "X_train = X_train.drop(columns=[column])\n",
    "X_train = pd.concat([X_train,pd.DataFrame(new_columns_train)], axis = 1)\n",
    "\n",
    "# Test\n",
    "#X_test[column] = breed_label.fit_transform(breed_test[column].values)\n",
    "new_columns_test = breed_binary.transform(breed_train[column].values.reshape(-1,1))\n",
    "X_test = X_test.drop(columns=[column])\n",
    "X_test = pd.concat([X_test,pd.DataFrame(new_columns_test)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AgeuponOutcome</th>\n",
       "      <th>IsDog</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>Mix</th>\n",
       "      <th>0_0</th>\n",
       "      <th>0_1</th>\n",
       "      <th>0_2</th>\n",
       "      <th>0_3</th>\n",
       "      <th>0_4</th>\n",
       "      <th>0_5</th>\n",
       "      <th>0_6</th>\n",
       "      <th>0_7</th>\n",
       "      <th>0_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.396792</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.396792</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.059510</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.714682</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.059510</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AgeuponOutcome  IsDog    0    1    2    0    1    2  Mix  0_0  0_1  0_2  \\\n",
       "0       -0.396792      1  0.0  1.0  0.0  0.0  1.0  0.0    1    0    0    0   \n",
       "1       -0.396792      0  0.0  1.0  0.0  1.0  0.0  0.0    1    0    0    0   \n",
       "2       -0.059510      1  0.0  1.0  0.0  0.0  1.0  0.0    1    0    0    0   \n",
       "3       -0.714682      0  1.0  0.0  0.0  0.0  1.0  0.0    1    0    0    0   \n",
       "4       -0.059510      1  0.0  1.0  0.0  0.0  1.0  0.0    1    0    0    0   \n",
       "\n",
       "   0_3  0_4  0_5  0_6  0_7  0_8  \n",
       "0    0    0    0    0    0    1  \n",
       "1    0    0    0    0    1    0  \n",
       "2    0    0    0    0    1    1  \n",
       "3    0    0    0    0    1    0  \n",
       "4    0    0    0    1    0    0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "**Question 15: Complétez pipeline ci-dessous (4 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from preprocessing import TransformationWrapper\n",
    "from preprocessing import LabelEncoderP\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from category_encoders import BinaryEncoder\n",
    "\n",
    "def parse_age(text):\n",
    "    time = 0.0\n",
    "    multiplier, timeframe = text.split(\" \")\n",
    "    multiplier = float(multiplier)\n",
    "    if \"day\" in timeframe:\n",
    "        time = multiplier\n",
    "    elif \"week\" in timeframe:\n",
    "        time = multiplier * 7.0\n",
    "    elif \"month\" in timeframe:\n",
    "        time = multiplier * (365.25/12.0)\n",
    "    elif \"year\" in timeframe:\n",
    "        time = multiplier * 365.25\n",
    "    return time\n",
    "\n",
    "def parse_state(text):\n",
    "    if \"Unknown\" in text:\n",
    "        return \"Unknown\"\n",
    "    state, _ = text.split(\" \")\n",
    "    if \"Spayed\" in state:\n",
    "        state = \"Neutered\"\n",
    "    return state\n",
    "\n",
    "def parse_sex(text):\n",
    "    if \"Unknown\" in text:\n",
    "        return \"Unknown\"\n",
    "    _, sex = text.split(\" \")\n",
    "    return sex\n",
    "\n",
    "def parse_mix(text):\n",
    "    return 1 if (text.endswith(\" Mix\") or \"/\" in text) else 0\n",
    "\n",
    "def remove_mix(text):\n",
    "    return text[:-4] if text.endswith(\" Mix\") else text\n",
    "\n",
    "def remove_slash(text):\n",
    "    return text[:text.find('/')] if '/' in text else text\n",
    "\n",
    "PATH = \"data/\"\n",
    "X_train = pd.read_csv(PATH + \"train.csv\")\n",
    "X_test = pd.read_csv(PATH + \"test.csv\")\n",
    "X_train = X_train.drop(columns = [\"OutcomeSubtype\",\"AnimalID\"])\n",
    "X_test = X_test.drop(columns = [\"ID\"])\n",
    "X_train, y_train = X_train.drop(columns = [\"OutcomeType\"]),X_train[\"OutcomeType\"]\n",
    "\n",
    "X_train1 = pd.read_csv(\"data/train_preprocessed.csv\")\n",
    "X_test1 = pd.read_csv(\"data/test_preprocessed.csv\")\n",
    "\n",
    "X_train = X_train.drop(columns = [\"Color\",\"Name\",\"DateTime\"])\n",
    "X_test = X_test.drop(columns = [\"Color\",\"Name\",\"DateTime\"])\n",
    "\n",
    "pipeline_age = Pipeline([\n",
    "    ('fillnan', SimpleImputer(strategy = 'constant', fill_value = '1 year')),\n",
    "    ('age', TransformationWrapper(transformation = parse_age)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "pipeline_animal = Pipeline([\n",
    "    ('encode', LabelEncoderP())\n",
    "])\n",
    "\n",
    "pipeline_state = Pipeline([\n",
    "    ('state', TransformationWrapper(transformation = parse_state)),\n",
    "    ('encode', LabelEncoderP()),\n",
    "    ('onehot', OneHotEncoder(categories = 'auto', sparse = False))\n",
    "])\n",
    "\n",
    "pipeline_sex = Pipeline([\n",
    "    ('sex', TransformationWrapper(transformation = parse_sex)),\n",
    "    ('encode', LabelEncoderP()),\n",
    "    ('onehot', OneHotEncoder(categories = 'auto', sparse = False))\n",
    "])\n",
    "\n",
    "pipeline_sexuponoutcome = Pipeline([\n",
    "    ('fillnan', SimpleImputer(strategy = 'constant', fill_value = 'Unknown')),\n",
    "    ('feats', FeatureUnion([\n",
    "        ('state', pipeline_state),\n",
    "        ('sex', pipeline_sex)\n",
    "    ]))\n",
    "])\n",
    "\n",
    "pipeline_mix = Pipeline([\n",
    "    ('mix', TransformationWrapper(transformation = parse_mix)),\n",
    "])\n",
    "\n",
    "pipeline_breed_encode = Pipeline([\n",
    "    ('mix', TransformationWrapper(transformation = remove_mix)),\n",
    "    ('slash', TransformationWrapper(transformation = remove_slash)),\n",
    "    ('binary', BinaryEncoder(impute_missing=True, handle_unknown='impute'))\n",
    "])\n",
    "\n",
    "pipeline_breed = Pipeline([\n",
    "    ('fillnan', SimpleImputer(strategy = 'constant', fill_value = 'Unknown')),\n",
    "    ('feats', FeatureUnion([\n",
    "        ('mix', pipeline_mix),\n",
    "        ('breed_encoding', pipeline_breed_encode)\n",
    "    ]))\n",
    "]\n",
    ")\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('AgeuponOutcome', pipeline_age, [\"AgeuponOutcome\"]),\n",
    "    ('AnimalType', pipeline_animal, [\"AnimalType\"]),\n",
    "    ('SexuponOutcome', pipeline_sexuponoutcome, [\"SexuponOutcome\"]),\n",
    "    ('Breed', pipeline_breed, [\"Breed\"]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lancez le pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\preprocessing\\label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\preprocessing\\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\preprocessing\\label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\preprocessing\\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\preprocessing\\label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\preprocessing\\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\preprocessing\\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\preprocessing\\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\preprocessing\\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#columns = [\"age\", \"isDog\", \"stateIntact\", \"stateNeutered\", \"stateUnknown\", \"sexFemale\", \"sexMale\", \"sexUnknown\"]\n",
    "X_train_prepared = pd.DataFrame(full_pipeline.fit_transform(X_train))\n",
    "X_test_prepared = pd.DataFrame(full_pipeline.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concaténation des deux parties du dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train1,X_train_prepared], axis = 1)\n",
    "X_test = pd.concat([X_test1,X_test_prepared], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model selection (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encodage de la classe cible sous forme d'entiers pour l'utiliser\n",
    "avec les algorithmes de scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adoption' 'Died' 'Euthanasia' 'Return_to_owner' 'Transfer']\n",
      "[3 2 0 ... 0 4 4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "target_label = LabelEncoder()\n",
    "y_train_label = target_label.fit_transform(y_train)\n",
    "print(target_label.classes_)\n",
    "print(y_train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble de validation\n",
    "Pour comparer différents modèles entre eux, on ne peut pas utiliser\n",
    "l'ensemble de test, sinon on serait tenté de garder le modèle correspondant le mieux à l'ensemble de test ce qui pourrait conduire à l'overfitting.\n",
    "\n",
    "Il est d'usage de créer un nouvel ensemble de la taille de l'ensemble de test, l'ensemble de **validation**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "La cross-validation est une méthode utile pour comparer la performance de différents modèles de machine learning **sans créer d'ensemble de validation**.\n",
    "\n",
    "Il existe différents types de cross-validation, la procédure la plus classique est la suivante:\n",
    "* Diviser aléatoirement l'ensemble d'entraînement en deux parties (90%/10% par exemple).\n",
    "* Entraîner le modèle sur la plus grande partie, et le tester sur l'autre partie.\n",
    "* Recommencer n fois\n",
    "* Calculer la moyenne et l'écart type des résultats\n",
    "\n",
    "Les avantages sont les suivants:\n",
    "* Considérer la totalité de l'ensemble d'entraînement pour l'évaluation (sans se priver de l'ensemble de validation)\n",
    "* Obtenir l'écart-type des résultats permet une meilleure évaluation de la précision du modèle.\n",
    "\n",
    "L'inconvénient principal est le temps de calcul, étant donné que l'on effectue l'apprentissage du modèle plusieurs fois, cette méthode peut être impossible pour des datasets contenant un grand nombre d'exemple (> 10e5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 2: StratifiedKFold (1 point)\n",
    "\n",
    "En observant la distribution des classes de l'attribut cible (à l'aide des fonctions de visualisation de pandas), justifiez l'utilisation de l'objet **StratifiedKFold** de sklearn pour la division de l'ensemble d'entraînement lors de cross-validation en comparaison à une méthode pûrement **aléatoire**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adoption' 'Died' 'Euthanasia' 'Return_to_owner' 'Transfer']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2326ea4fe80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(target_label.classes_)\n",
    "pd.Series(y_train_label).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification ###\n",
    "On voit que la distribution des classes est très inégale dans l'ensemble d'entraînement. En effet, les classes *Adopted* et *Transfer* représentent la très grande majorité des données, alors que les classes *Died* et *Euthanasia* sont très peu présentes. Utiliser une méthode aléatoire pour l'attribution des ensembles de validation amènerait une disparité importante au niveau des données d'entraînement, ce qui pousserait le modèle à se surentraîner sur des données très peu présentes (celles qui représentent les animaux décédés par exemple), et se sous-entraîner sur les données qui se retrouvent les plus fréquemment dans le dataset. En utilisant *StratifiedKFold*, qui va effectuer des divisions **représentatives** des données d'entraînement pour la validation, on essaie de faire en sorte que les modèles valident leurs prédictions sur des données qui sont représentatives de l'ensemble d'entraînement sélectionné. On le fait pour un certain nombre de \"strates\" différentes (des divisions subséquentes qui sont le plus possible proportionnelles, voir https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py) afin de s'assurer d'avoir une certaine cohérence entre les exécutions (on fera des moyennes et des écarts-types dans l'analyse de la question 16)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 16: (1 point)\n",
    "\n",
    "\n",
    "**Choisir au moins deux modèles permettant la classification multiclasse sur sklearn en plus du modèle implémenté dans la première partie du TP**.\n",
    "\n",
    "**Complétez la fonction compare qui effectue la crossvalidation pour différents modèles et différentes métriques, et renvoie la liste des moyennes et écart-types pour chacune des métriques, pour chacun des modèles. **\n",
    "\n",
    "**En vous basant sur les différentes métriques, concluez quant au modèle le plus performant.**\n",
    "\n",
    "Evaluez les modèles pour les différentes métriques proposées:\n",
    "* **log loss**: c'est la métrique d'évaluation de kaggle\n",
    "* **precision**: correspond à la qualité de la prédiction, le nombre de classes correctement prédites par le nombre de prédiction total\n",
    "* **recall**: le nombre d'éléments appartenant à une classe, identifiés comme tel, divisé par le nombre total des éléments de cette classe.\n",
    "* **f-score**: une moyenne de la precision et du recall\n",
    "\n",
    "**Remarque: precision et recall sont deux mesures complémentaires pour l'évaluation d'un modèle de classification multi-classe.**\n",
    "\n",
    "Dans le cas d'une classification binaire avec un déséquilibre de la classe cible important, (90%/10%), en évaluant le résultat de la classification avec l'accuracy (nombre de prédictions correctes divisé par le nombre de prédictions total), on peut obtenir un très bon score (90% d'accuracy) en choisissant de prédire systématiquement la classe majoritaire.\n",
    "\n",
    "Dans un tel cas, la precision serait élevée de même, mais le recall serait très bas , nous indiquant la médiocrité de notre modèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "import numpy as np\n",
    "\n",
    "def compare(models,X_train,y_train,nb_runs, scoring):\n",
    "    losses = []\n",
    "    for model in models:\n",
    "        scores = cross_validate(model, X_train, y_train, cv=nb_runs, scoring=scoring, return_train_score=False)\n",
    "        avg_log_loss = np.mean(-scores['test_neg_log_loss'])\n",
    "        std_log_loss = np.std(-scores['test_neg_log_loss'])\n",
    "        avg_precision = np.mean(scores['test_precision_macro'])\n",
    "        std_precision = np.std(scores['test_precision_macro'])\n",
    "        avg_recall = np.mean(scores['test_recall_macro'])\n",
    "        std_recall = np.std(scores['test_recall_macro'])\n",
    "        avg_fscore = np.mean(scores['test_f1_macro'])\n",
    "        std_fscore = np.std(scores['test_f1_macro'])\n",
    "        losses += [{'model' : model,\n",
    "                   'avg_log_loss' : avg_log_loss,\n",
    "                   'std_log_loss' : std_log_loss,\n",
    "                   'avg_precision' : avg_precision,\n",
    "                   'std_precision' : std_precision,\n",
    "                   'avg_recall' : avg_recall,\n",
    "                   'std_recall' : std_recall,\n",
    "                   'avg_fscore' : avg_fscore,\n",
    "                   'std_fscore' : std_fscore}]\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'model': SoftmaxClassifier(alpha=1, early_stopping=True, eps=1e-05, lr=0.1,\n",
       "           n_epochs=1000, regularization=True, threshold=1e-10,\n",
       "           use_zero_indexed_classes=True),\n",
       "  'avg_log_loss': 1.4619168007080647,\n",
       "  'std_log_loss': 0.12620434116207813,\n",
       "  'avg_precision': 0.2559444133872813,\n",
       "  'std_precision': 0.07862107842945894,\n",
       "  'avg_recall': 0.22146496690354978,\n",
       "  'std_recall': 0.03404575294613686,\n",
       "  'avg_fscore': 0.17485259143650767,\n",
       "  'std_fscore': 0.04497381168525315},\n",
       " {'model': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "              max_depth=13, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
       "              oob_score=False, random_state=None, verbose=0,\n",
       "              warm_start=False),\n",
       "  'avg_log_loss': 0.8209776378321451,\n",
       "  'std_log_loss': 0.005659185590009484,\n",
       "  'avg_precision': 0.6139219537986745,\n",
       "  'std_precision': 0.08465669083407668,\n",
       "  'avg_recall': 0.4052930243889335,\n",
       "  'std_recall': 0.006150833908451374,\n",
       "  'avg_fscore': 0.4135937504578879,\n",
       "  'std_fscore': 0.008527561502294561},\n",
       " {'model': MLPClassifier(activation='relu', alpha=0.1, batch_size='auto', beta_1=0.9,\n",
       "         beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "         hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "         learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "         n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "         random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "         validation_fraction=0.1, verbose=False, warm_start=False),\n",
       "  'avg_log_loss': 0.8431417387591625,\n",
       "  'std_log_loss': 0.010010133261444459,\n",
       "  'avg_precision': 0.4942328692868588,\n",
       "  'std_precision': 0.010528745687545577,\n",
       "  'avg_recall': 0.4072226583372834,\n",
       "  'std_recall': 0.0106564799445202,\n",
       "  'avg_fscore': 0.4149219814984512,\n",
       "  'std_fscore': 0.011177325087799197}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from SoftmaxClassifier import SoftmaxClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nb_run = 3\n",
    "\n",
    "models = [\n",
    "    SoftmaxClassifier(alpha=1, use_zero_indexed_classes=True),\n",
    "    RandomForestClassifier(criterion='entropy', max_depth=13, max_features='auto', n_estimators=500),\n",
    "    MLPClassifier(alpha=0.1)\n",
    "]\n",
    "\n",
    "scoring = ['neg_log_loss', 'precision_macro','recall_macro','f1_macro']\n",
    "\n",
    "compare(models,X_train,y_train_label,nb_run,scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification ###\n",
    "Suite aux résultats obtenus, nous avons choisi le classificateur *RandomForestClassifier* pour l'apprentissage du dataset. En effet, avec un peu d'ajustement des hyper-paramètres, ce classificateur obtient des résultats relativement corrects en un temps très court par rapport aux deux autres. On observe notamment :\n",
    "* Le *log-loss* est le plus petit des trois classificateurs testés.\n",
    "* La précision est de loin la meilleure (~ 0.61) mais avec une forte variance; la performance dépend donc de la façon dont sont générées les strates KFold.\n",
    "* Le *recall* n'est pas le meilleur, mais se rapproche de celui du MLP tout en étant relativement stable (peu de variance).\n",
    "* Le F-score n'est également pas le meilleur ni le plus invariant, mais la rapidité d'entraînement du classificateur et les résultats concrets observés à la question 17 confirment le choix de celui en contrebalançant le désavantage par rapport au MLP. Par exemple, le *RandomForest* est capable de prédire le décès d'un certain nombre d'animaux, ce que le MLP est incapable de faire (il n'en prédit aucun); cette classe pourrait être considérée comme importante à prédire correctement.\n",
    "\n",
    "Le classificateur Softmax offre une performance décevante; cela est probablement dû au fait qu'on ne se base que sur 5 \"neurones\" de sortie pour apprendre les nombreuses *features* (beaucoup de poids sont associés à chaque neurone). On peut observer ce comportement par le *log-loss* très important de ce classificateur. Pour contrebalancer ce désavantage, il faudrait effectuer de nombreuses *epoch* (bien plus que 1000), en plus d'ajuster très finement les hyper-paramètres, ce qui demande beaucoup de temps.\n",
    "\n",
    "De même, le MLP est un classificateur très intéressant, mais il requiert un temps important pour s'entraîner, et le peaufinage des hyper-paramètres est plus difficile qu'avec le *RandomForestClassifier* (surtout lorsqu'on utilise *GridSearchCV* pour tester les différents hyper-paramètres de manière exhaustive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 17: Matrice de confusion (0.5 point)\n",
    "\n",
    "La matrice de confusion A est telle que $A_{i,j}$ correspond au nombre d'exemples de la classe i classifié comme appartenant à la classe j.\n",
    "\n",
    "Entrainez le modèle sélectionné sur la totalité de l'ensemble d'entraînement.\n",
    "A l'aide de la matrice de confusion et de la distribution des classes, analysez plus en détail les performances du modèle choisi et justifiez les."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train selected model\n",
    "\n",
    "selected_model = RandomForestClassifier(criterion='entropy', max_depth=12, max_features='auto', n_estimators=400)\n",
    "selected_model.fit(X_train, y_train_label)\n",
    "y_pred = selected_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adoption</th>\n",
       "      <th>Died</th>\n",
       "      <th>Euthanasia</th>\n",
       "      <th>Return_to_owner</th>\n",
       "      <th>Transfer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Adoption</th>\n",
       "      <td>9915</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>570</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Died</th>\n",
       "      <td>19</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Euthanasia</th>\n",
       "      <td>251</td>\n",
       "      <td>0</td>\n",
       "      <td>373</td>\n",
       "      <td>307</td>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Return_to_owner</th>\n",
       "      <td>2126</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2434</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transfer</th>\n",
       "      <td>2364</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>524</td>\n",
       "      <td>6530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Adoption  Died  Euthanasia  Return_to_owner  Transfer\n",
       "Adoption             9915     0           0              570       284\n",
       "Died                   19    33           0                8       137\n",
       "Euthanasia            251     0         373              307       624\n",
       "Return_to_owner      2126     0           3             2434       223\n",
       "Transfer             2364     0           4              524      6530"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pd.DataFrame(confusion_matrix(y_train_label, y_pred), columns = target_label.classes_, index = target_label.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Distribution des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adoption' 'Died' 'Euthanasia' 'Return_to_owner' 'Transfer']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt \n",
    "print(target_label.classes_)\n",
    "pd.Series(y_train_label).hist()\n",
    "\n",
    "# Fait selon https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "cm = confusion_matrix(y_train_label, y_pred)\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "f = plt.figure(figsize=(15, 5))\n",
    "f.add_subplot(1,2, 1)\n",
    "plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Greens)\n",
    "plt.title(\"Confusion matrix in graph form\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(target_label.classes_))\n",
    "plt.xticks(tick_marks, target_label.classes_, rotation=45)\n",
    "plt.yticks(tick_marks, target_label.classes_)\n",
    "\n",
    "fmt = 'd'\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], fmt),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "f.add_subplot(1,2, 2)\n",
    "plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Greens)\n",
    "plt.title(\"Confusion matrix in graph form normalized\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(target_label.classes_))\n",
    "plt.xticks(tick_marks, target_label.classes_, rotation=45)\n",
    "plt.yticks(tick_marks, target_label.classes_)\n",
    "\n",
    "fmt = '.2f'\n",
    "thresh = cm_norm.max() / 2.\n",
    "for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "    plt.text(j, i, format(cm_norm[i, j], fmt),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm_norm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "plt.show(block=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification ###\n",
    "On voit que le classificateur RandomForestClassifier avec les hyper-paramètres choisis se débrouille plutôt bien pour certaines classes. En effet, voici les observations pour chaque classe :\n",
    "* Les animaux adoptés sont bien prédits 92% du temps. Il arrive parfois que le modèle se trompe et fasse une prédiction comme quoi l'animal est retourné chez son propriétaire (5%) ou transféré (3%).\n",
    "* Les animaux qui sont décédés sont mal prédits. En effet, le modèle prédit plutôt un transfert la majorité du temps (70%). Toutefois, comme le dataset contient peu d'exemplaires de cette classe (voir histogramme), il peut être difficile pour le modèle de bien la prédire. On remarque en revanche qu'il est très pessimiste (prudent), et qu'il n'a jamais prédit qu'un animal était décédé alors qu'il ne l'était pas.\n",
    "* Les animaux euthanasiés subissent environ le même sort que ceux qui sont décédés, mais le modèle est plus incertain quant à leur classe; il attribue de 16% (adoption) à 41% (transfert) de prédictions aux autres classes, contre 24% pour la bonne classe (euthanasie).\n",
    "* Le retour au propriétaire est bien prédit une faible majorité du temps (51%). Sinon, le modèle prédit une adoption (44%) ou un transfert (5%). On peut déduire que la différence entre l'adoption et le retour au propriétaire n'est pas très nette, surtout si l'on compare aux résultats pour les animaux ayant été adoptés (ligne 1), où les classes mal prédites sont presque toujours classées comme un retour au propriétaire.\n",
    "* Le transfert est bien prédit la majorité du temps (69%). Le modèle se confond encore ici avec une adoption (25%) ou un retour au propriétaire (6%).\n",
    "\n",
    "On remarque que les classes ayant une présence accrue (adoption et transfert, surtout) sont relativement bien prédites, alors que celles se retrouvant en moins grand nombre dans le dataset sont plus vagues (retour au propriétaire, euthanasie), ou carrément mal prédites (décès). Celles qui sont mal prédites le sont de façon pessimiste (le modèle n'attribue presque jamais une prédiction à ces classes lorsqu'il est en présence d'une autre), ce qui pourrait indiquer qu'il a du mal à corréler des facteurs qui causent explicitement des animaux à être classés de telle façon. Il faut se rappeler que les arbres de décision (dont est composée la RandomForest) ont besoin de features qui ont un apport clair sur la décision finale pour être efficaces (ils tentent de réduire l'entropie apportée par chaque feature), ce qui n'est peut-être pas le cas de notre dataset transformé.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 3: Optimisation des hyper-paramètres (1 point)\n",
    "\n",
    "Les hyper-paramètres sont les paramètres fixés avant la phase d'apprentissage. Pour optimiser les performances du modèle, on peut sélectionner les meilleurs hyper-paramètres.\n",
    "\n",
    "A l'aide de sklearn, optimisez les hyper-paramètres du modèle que vous avez sélectionné et montrez que les performances ont été améliorées.\n",
    "Vous pouvez utiliser par exemple: **GridSearchCV**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 13, 'max_features': 'auto', 'n_estimators': 500}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'model': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "              oob_score=False, random_state=None, verbose=0,\n",
       "              warm_start=False),\n",
       "  'avg_log_loss': 3.1514805499756533,\n",
       "  'std_log_loss': 0.07499495760768035,\n",
       "  'avg_precision': 0.47314985039962393,\n",
       "  'std_precision': 0.012205017168963625,\n",
       "  'avg_recall': 0.4121115689924906,\n",
       "  'std_recall': 0.0005818074417184664,\n",
       "  'avg_fscore': 0.4256243278782514,\n",
       "  'std_fscore': 0.002164028462592549},\n",
       " {'model': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "              max_depth=13, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
       "              oob_score=False, random_state=None, verbose=0,\n",
       "              warm_start=False),\n",
       "  'avg_log_loss': 0.8221261809566592,\n",
       "  'std_log_loss': 0.005516457378355828,\n",
       "  'avg_precision': 0.619358793508292,\n",
       "  'std_precision': 0.08073630836210204,\n",
       "  'avg_recall': 0.4057856104302549,\n",
       "  'std_recall': 0.006142176111249419,\n",
       "  'avg_fscore': 0.4142934063645772,\n",
       "  'std_fscore': 0.008390522213506472}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from SoftmaxClassifier import SoftmaxClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "def compare(models,X_train,y_train,nb_runs, scoring):\n",
    "    losses = []\n",
    "    for model in models:\n",
    "        scores = cross_validate(model, X_train, y_train, cv=nb_runs, scoring=scoring, return_train_score=False)\n",
    "        avg_log_loss = np.mean(-scores['test_neg_log_loss'])\n",
    "        std_log_loss = np.std(-scores['test_neg_log_loss'])\n",
    "        avg_precision = np.mean(scores['test_precision_macro'])\n",
    "        std_precision = np.std(scores['test_precision_macro'])\n",
    "        avg_recall = np.mean(scores['test_recall_macro'])\n",
    "        std_recall = np.std(scores['test_recall_macro'])\n",
    "        avg_fscore = np.mean(scores['test_f1_macro'])\n",
    "        std_fscore = np.std(scores['test_f1_macro'])\n",
    "        losses += [{'model' : model,\n",
    "                   'avg_log_loss' : avg_log_loss,\n",
    "                   'std_log_loss' : std_log_loss,\n",
    "                   'avg_precision' : avg_precision,\n",
    "                   'std_precision' : std_precision,\n",
    "                   'avg_recall' : avg_recall,\n",
    "                   'std_recall' : std_recall,\n",
    "                   'avg_fscore' : avg_fscore,\n",
    "                   'std_fscore' : std_fscore}]\n",
    "    return losses\n",
    "\n",
    "# Perform GridSearchCV\n",
    "parameters = {'n_estimators': [400, 500], 'max_features':['auto'], 'criterion': ['gini', 'entropy'], 'max_depth': [10, 11, 12, 13, 14, 15]}\n",
    "rfc = RandomForestClassifier(random_state=69)\n",
    "rfc_cv = GridSearchCV(rfc, parameters, cv=3, scoring='neg_log_loss')\n",
    "\n",
    "rfc_cv.fit(X_train, y_train)\n",
    "print(rfc_cv.best_params_)\n",
    "\n",
    "nb_run = 3\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(),\n",
    "    RandomForestClassifier(criterion='entropy', max_depth=13, max_features='auto', n_estimators=500),\n",
    "]\n",
    "\n",
    "scoring = ['neg_log_loss', 'precision_macro','recall_macro','f1_macro']\n",
    "\n",
    "compare(models,X_train,y_train_label,nb_run,scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 18: Soumission (0.5 point)\n",
    "\n",
    "Enfin, effectuez la prédiction sur l'ensemble de test et joignez vos résultats au rendu du TP.\n",
    "\n",
    "**Optionnel**: Vous pouvez soumettre vos résultats sur kaggle et noter votre performance en terme de log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = RandomForestClassifier(criterion='entropy', max_depth=13, max_features='auto', n_estimators=500)\n",
    "best_model.fit(X_train, y_train)\n",
    "pred_test = pd.Series(best_model.predict(X_test))\n",
    "pred_test.to_csv(\"test_prediction.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
